---
layout: '../../../layouts/ChildSectionAiRadar.astro'
title: 'Platforms: Trial'
---

# Trial

These platforms show promising potential with growing adoption and active development. While they may not yet have the same maturity as Adopt platforms, they offer innovative approaches and capabilities that make them worth exploring for forward-thinking teams.

<div data-radar data-meta='{"label":"MLflow"}' />

## MLflow

We have placed MLFlow in the Trial ring of the Platforms quadrant due to its potential as a lightweight and modular option for teams seeking to manage the machine learning lifecycle. Its open-source nature makes it an attractive alternative to the more monolithic cloud-based MLOps platforms provided by vendors like AWS, Microsoft and Google. A key advantage of MLFlow is its ability to avoid vendor lock-in, offering teams the flexibility to maintain control of their infrastructure and adapt workflows as their needs evolve.

That said, realising the benefits of MLFlow requires teams to have a certain level of technical expertise to configure and integrate it into their existing systems effectively. Unlike cloud-native behemoths such as SageMaker or Vertex AI, MLFlow does not provide an all-in-one, plug-and-play experience. Instead, it offers modular components that must be tailored to specific use cases. We recommend assessing MLFlow if your organisation values flexibility, has the technical proficiency to manage integrations, and prefers avoiding dependency on proprietary platforms early in your MLOps journey.

<div data-radar data-meta='{"label":"Open weight LLMs"}' />

## Open weight LLMs

2024 was the year when open weight LLMs (which are sometimes incorrectly referred to as 'open source') from companies such as Meta and Deepseek reached maturity, with some even surpassing flagship frontier models on certain tasks. We've placed open weight LLMs in the Trial ring because they allow organisations to benefit from AI capabilities while maintaining control over their data and deployment. These models have demonstrated impressive performance, particularly in specialised domains when fine-tuned on specific tasks.

The key benefits include reduced operational costs compared to API-based services, full control over model deployment and customisation, and the ability to run models in air-gapped environments where data privacy is paramount. However, we've kept them in Trial because organisations need considerable ML engineering expertise to deploy and maintain these models effectively, and the total cost of ownership isn't always lower than API-based alternatives when accounting for computational resources and engineering time.

For certain use cases, the simplicity of a pay-per-use API integration outweighs the benefits and greater control of hosting an open source LLM. Additionally, implementing appropriate security controls, prompt injection protection, and data governance poses significant challenges.

<div data-radar data-meta='{"label":"Lakera"}' />

## Lakera

Lakera is an AI safety and robustness platform designed to detect and mitigate risks in machine learning systems. It provides mechanisms for testing, analysis, and quality assurance to help developers identify weaknesses or vulnerabilities in AI/ML models prior to deployment. This makes it particularly appealing in contexts where reliability and safety are paramount, such as finance, healthcare, or any domain subject to compliance constraints.

We have placed Lakera in the Trial ring because its value is clearly demonstrated in scenarios where established safety practices need to be extended to AI systems. Its ability to identify edge cases and potential failures adds a layer of assurance that is crucial for risk-sensitive operations. However, as a newer and relatively niche platform, it is still finding its place within mainstream AI development processes. Teams looking to leverage its capabilities will need to have a certain level of maturity in their own testing and validation workflows.

For now, Lakera stands out as a focused and valuable tool for teams committed to addressing AI risk in the most responsible way. We recommend conducting small-scale experiments to assess its suitability for your specific requirements before considering broader adoption.

<div data-radar data-meta='{"label":"Vector Databases"}' />

## Vector Databases

Vector databases have emerged as specialised tools for managing the high-dimensional data representations (embeddings) required by AI models. They enable efficient similarity search across text, images, and other content types. Prominent solutions include Pinecone, Qdrant, Milvus 2.0 and Weaviate.

We've generally placed vector databases in the Trial ring, as they have proven valuable for specific use cases such as semantic search and recommendation systems. However, their adoption should be carefully evaluated based on individual requirements. Traditional databases may be sufficient for simpler operations and typically require less coordination. Alternative approaches, such as Timescale's PGAI vectorizer, bring vector embedding search directly into the Postgres database, ensuring embeddings remain synchronised with underlying content changes.

If a vector database is required for your use case, the choice of provider often depends on factors such as scale requirements, the need for real-time updates, and whether a managed or self-hosted solution is preferred. Pinecone leads in production readiness but comes with the costs of a managed service, while open-source alternatives like Qdrant and Milvus offer greater control but demand more operational expertise.
