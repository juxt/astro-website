---
token: '1234'
author: 'jam'
layout: '../../layouts/BlogPost.astro'
title: 'A Primer on Logic Programming'
description: "Getting a kickstart in the logic programming world with Prolog"
publishedDate: '2023-05-19'
tags:
  - prolog
  - logic
heroImage: 'wam.jpg'
---

I think the natural response to learning about formal logic for the first time is to roll one's eyes. For many, formal logic is typical of the impractical rigor of academia and its obsession with formalisms; an exercise in pure semantics- *surely, simply messing around with statements about what __is__, can never tell us what it is that we can __do__*- and so, I believe this attitude contributes to the now-esoteric state of logic programming as a whole.

From my own experience, I can say that learning logic programming, which as a functional programmer I had originally regarded as stale, pointless, dull - was as much of a paradigm leap for me as the step from imperative to functional. It's not intuitive. But after a while I began to realise that not only did logical and relational languages allow me to express ideas in a concise declarative manner, but that I could take the knowledge and insights I gained from my studies *back* to the functional world to power my abstractions , DSLs, and algorithms. I'd like to teach and demonstrate its (at least pedagogical) value in this article.

To begin with, let's consider Prolog- one of the oldest and most well-known logic languages (and the progenitor of Erlang, and that story is [fascinating in its own right](https://www.semanticscholar.org/paper/A-history-of-Erlang-Armstrong/20588077be87593e4a6e0efe2229bda8488f05eb)). The basic unit of abstraction in Prolog is a [Horn Clause](https://athena.ecs.csus.edu/~mei/logicp/prolog.html) as opposed to a function. Horn clauses consist of two parts:

1. The 'head' (also called a goal clause), which is constituted by the name of the predicate we are defining and its arity- the number of arguments the predicate takes
2. The 'body', which can be considered to be 'conditionals'- if all the predicates in the body are true, then the head is true (equivalent to logical 'and'). 

Let's consider a couple of examples. 
```
Parent(John, Jill).
Parent(John, Jack).
```
In this example, we are asserting the predicate John is the parent of Jill and that John is also the parent of Jack, without any conditions under which it is true- hence it is true by default, and is what we call a 'fact'. Prolog begins with a set of 'facts', that it knows to be true. When we define this predicate, Prolog adds the fact to its environment, to be used later. We can also define multiple definitions for the predicate, and this is equivalent to a logical 'or'- Parent is true if the first argument is John and the second argument is Jill, OR if the first argument is John and the second is Jack. Now let's start to define what it means to be an 'ancestor'.
```
Ancestor(X, Y) :- Parent(X, Y).
```
This statement means that to be an ancestor (the goal) is to be a parent (the condition). That definition is a little lacking, but with recursion, we can make it better.
```
Ancestor(X, Y) :- Parent(X, Y).
Ancestor(X, Y) :- Parent(X, Z), Ancestor(Z, Y).
```
By introducing the intermediate variable Z in the body, we can solve the problem. Now we're asserting that to be an ancestor means either that X is the parent of Y, but it also could mean that X is the parent of someone who has an ancestral relation to Y. That is logically what it means to be an ancestor. In first-order logic we might write this as 
```
Ancestor(X, Y) → Parent(X, Y) ∨ (Parent(X, Z) ∧ Ancestor(Z, Y)).
```
 But how does this code actually execute? We haven't actually implemented any code to say how we find the ancestor - we have only said what an ancestor *is*! The trick lies in how Prolog code is executed. In a functional language, the compiled machine code more or less boils down to 'calculate a bunch of things and return the result of those calculations', and this is how the interpreter of that machine code works- by crunching numbers. In Prolog, and in logic languages in general, it's fundamentally different. Prolog relies upon the Warren Abstract Machine (WAM), the literature for which is [highly dense and formal](https://www.researchgate.net/publication/220986765_Warren's_Abstract_Machine_A_Tutorial_Reconstruction), but here is a simple explanation. The WAM is a stack-based abstract machine with several components:

1. The current environment, implemented as a stack
2. The heap, which stores terms.
3. The trail, which keeps tracks of changes to variables (E.G., binding of X to John) for the purposes of backtracking and undoing changes

The core way in which the WAM derives the correct results from our clauses is via 'search'. When we define 'Ancestor' the WAM stores information about 'Ancestor' in the heap, like the arity and the conditions. When we call 'Ancestor' with bound parameters (E.G., `Ancestor(John, Jill).`) a lookup is performed for 'Ancestor', after which X and Y are bound to John and Jill respectively (this is called 'unification', and is the same as how things work in pattern matching- if a pattern doesn't match the goal fails), and then another lookup is performed on Parent`(John, Jill)` which evaluates to true- if every condition in the body evaluates to true, then the predicate is true- goals are recursively evaluated. If the goal fails, the WAM will backtrack and try a different path to make the goal succeed. 

The Prolog interpreter can also use 'substitution' to derive new facts. For example if we call `Ancestor(John, X).` the interpreter is smart enough to retry executing the code with X instantiated as Jill and Jack- the goal is true if X is either of those two values. This allows the WAM to derive new facts for us, after which it will return those facts to us as results, if there are any. Peter Norvig builds a Prolog interpreter in Common Lisp that does this very thing [in his book](https://github.com/norvig/paip-lisp), so for anyone wanting a deeper hands-on understanding, I'd highly recommend it- but more or less now you should have everything you need to build a Prolog interpreter of your own. Once you have written a simple interpreter, you can enrich it with more interesting properties, and it's possible to write some *really fascinating* meta-interpreters in Prolog itself with [only a few lines of code](https://www.youtube.com/watch?v=nmBkU-l1zyc). 

I've always believed that the best way to improve at programming, especially when learning a new paradigm, is to solve puzzles- specifically, I think recursive puzzles are especially conducive to learning. So I'd like to take some of our favourite functional constructs (map, filter, reduce) and explain how we might write these in Prolog.

### Map
It's always best, when looking at solving a problem in a logic language, to start by considering the goal. In a functional language, we might say that *to* map is to calculate a linear transform between one data structure and another. But in Prolog, I would ask *what is a Mapping between two structures? And in what context?* A mathematician would look at two structures under a function and say that they are mapped if the two structures have the property of being *bijective under a function*. Bijectivity refers to the property that every element in the first structure has one-and-only-one corresponding element in the second structure. The second structure is a map of the first structure if every element corresponds to another element in some way- so that's our first insight, we can say that for every X in structure 1, and for every  corresponding Y in structure 2, there is some relation between them. What is that relation? It is nothing other than the mapping 'function' (which is actually a *predicate*). We should also consider our base case so that we have an endpoint. We can then derive Map to be
```
Map(_, [], []).
Map(P, [X|Xs], [Y|Ys]) :- Call(P, X, Y), Map(P, Xs, Ys).
```
In our base case, we have used the underscore to denote that we don't care what that value is- it can be any value. If the first list is empty, and the second list is empty, then that's a mapping. This is necessary- But when the lists are not empty, the case is more interesting. Firstly, notice that we can use destructuring on the lists. Secondly, we are using a built-in predicate called 'Call' (think apply), which simply applies a predicate to some arguments- and then we recur. Eventually the lists will be empty, if the lists are bijective. So this code says 'Map is true when the lists are empty, or when the supplied predicate is true for every element between the two lists'.

### Filter
Whereas maps are bijective due to the predicate being true for every corresponding element in List 1 and List 2, in filters that's not the case. Sometimes, the predicate fails. We can deal with this by adding an extra clause.
```
Filter(_, [], []).
Filter(P, [X|Xs], [X|Ys]) :- Call(P, X), Filter(P, Xs, Ys).
Filter(P, [X|Xs], Ys) :- Not(Call(P, X)), Filter(P, Xs, Ys).
```
This definition states that filter is true if the two lists are empty, OR the filter is true if for every element in list 1 there is a corresponding element in list 2 which is the same, and that element satisfies the given predicate P, OR that there is a filtration between the rest of List 1 and List 2. This is not entirely correct actually, because if we supply something like `Filter(Parent, X, [John])`, X could actually be any list that contains John- whereas this function will return simply `[John]`- we could simply argue that it makes no sense to try and call filter the inverse way around (it's cool that we can try!).  It's also worth nothing that there are some syntactic sugars that can help us define an inline if which shortens this definition, though it's unnecessary for our purposes.

### Reduce
Reduce brings us into the realm of using tail recursion. During a reduction in functional style, we call a function between an accumulator and each successive value until we build up the accumulator into our result. We can do something similar in Prolog. 
```
Reduce(Predicate, [], Result, Result).

Reduce(Predicate, [X|Xs], Acc, Result) :- Call(P, Acc, X, Acc2), Reduce(P, Xs, Acc2, Result).
```
Here we are defining an intermediate value which is the result when the predicate is true and called on the intermediate value of the last step in the reduction. For every element in the list, if the predicate is true when provided with the arguments of the element, the last intermediate value, and the new intermediate value, and if there's a reduction for the next element of the list, and the result does not change, then Reduce is true. The base case states that the list must be empty and the accumulator should be equivalent to the final result (which is correct). In my opinion this is the hardest example to grasp, due to the accumulator, and it's notable that again this is uni-directional (we cannot find the inverse of the reduction). But by grasping this example, you are well on your way to becoming a whizz logician!

### Conclusion
When we reason about a functional program in order to write code, we might say to ourselves 'How do I explain to the machine what calculations it must perform in order to achieve my desired goal?' But in logic languages, it's the inverse- we ask 'How do I explain to the machine my goal as correctly as possible such that the facts I want can be derived?' We can be a little more sure that our program is correct if it runs and returns a value, because we have put constraints on the result. I'm hoping that by giving you this new perspective on problem solving, you'll be able to apply it in your own reasoning about search, implement similar ideas in your DSLs, and maybe it will at least serve to give you a deeper understanding about technologies you already use, like SQL and XT Datalog. More important than any of those things though, I believe logic programming's greatest value is to help us realise how often we overestimate our own understanding of concepts we use on a daily basis. If interested in more, you can check out extensions to prolog such as [Mercury](https://mercurylang.org/about/motivation.html) and [Lambda Prolog](https://www.lix.polytechnique.fr/~dale/lProlog/).

