---
author: 'hga'
title: 'How do swarms invent?'
description: 'Architectural ideas outlast the code they''re embedded in. Now they propagate at LLM speed.'
category: 'ai'
layout: '../../layouts/BlogPost.astro'
publishedDate: '2026-02-24'
heroImage: 'how-do-swarms-invent.jpg'
tags:
  - 'ai'
  - 'agentic coding'
  - 'engineering'
  - 'architecture'
---

<p class="lede">In 1967, <a href="https://www.melconway.com/Home/Conways_Law.html" target="_blank">Melvin Conway</a> submitted a paper to the <em>Harvard Business Review</em> arguing that any organisation designing a system will produce a design that mirrors its own communication structure. The paper was called "How Do Committees Invent?"</p>

The committee [rejected it](https://www.melconway.com/Home/pdf/committees.pdf). Fred Brooks, author of [*The Mythical Man-Month*](https://en.wikipedia.org/wiki/The_Mythical_Man-Month), named it [Conway's Law](https://en.wikipedia.org/wiki/Conway%27s_law) a few years later, and six decades of evidence have made it hard to dispute. The law is usually read as a warning about structure: fix your org chart, fix your architecture. The deeper problem is what survives after the committee disbands. The assumptions a design team embeds in its work outlast the team, the technology and the reasons. Increasingly, the committee is a [swarm of agents](/blog/from-specification-to-stress-test) making architectural decisions in minutes. What persists when the swarm finishes?

## Good fences make good neighbours

LLMs write good code. By most measures, better than many humans. Give one an overarching goal and ask it to plan, and the architectural results are surprisingly coherent. [Research into agentic software architecture](https://arxiv.org/pdf/2509.08646) finds that a planning agent that decomposes a problem before worker agents execute consistently outperforms uncoordinated generation. Humans bring [cognitive biases](https://www.researchgate.net/publication/317433924_On_Cognitive_Biases_in_Architecture_Decision_Making) to architectural decision-making, from anchoring on familiar solutions to optimism about preferred approaches, and experienced practitioners are [more susceptible than students](https://arxiv.org/html/2502.04011v1). An LLM with a plan might have the edge in design as well as implementation.

<span class="pullquote" text-content="Simplification is hard, lonely work, and it doesn't parallelise."></span>That edge depends on the plan existing. Without deliberate upfront design, the defaults take over. An LLM working on its piece will happily generate a module that handles authentication, logging and billing in one place. A model whose neurons are [polysemantic by nature](https://transformer-circuits.pub/2023/monosemantic-features), each one responding to a [tangle of unrelated concepts](https://arxiv.org/abs/2505.11581), is not going to lose sleep over a component that does six things. Simplification is hard, lonely work, and it doesn't parallelise.

Rich Hickey makes the case that [easy and simple are not the same thing](https://www.infoq.com/presentations/Simple-Made-Easy/). Easy is whatever produces working output fastest, whatever you're used to doing. Simple is whatever keeps concerns separated. Simple might be very hard. Hickey's "easy" is Shakespeare's [primrose path](https://en.wikipedia.org/wiki/Primrose_path): the pleasant road to somewhere you'd rather not arrive. Tony Hoare framed the choice in his [1980 Turing Award lecture](https://dl.acm.org/doi/10.1145/358549.358561): "There are two ways of constructing a software design: One way is to make it so simple that there are obviously no deficiencies, and the other way is to make it so complicated that there are no obvious deficiencies."

## The primrose path

<span class="pullquote left" text-content="Easy is whatever the model generates most naturally. Simple requires deliberate subtraction that nobody specified."></span>Cursor's [FastRender](https://github.com/nickelcat/nickelcat-fast-render) showed where the path leads. Two thousand agents produced three million lines of entangled Rust that a Servo maintainer [called](https://simonwillison.net/2026/Jan/23/fastrender/) "a tangle of spaghetti", three times the size of [Servo](https://servo.org/) for a fraction of its capability. Easy is whatever the model generates most naturally. Simple requires deliberate subtraction that nobody specified.

*The Mythical Man-Month* explains why coordination at this scale fails. Two thousand agents would need [nearly two million communication channels](https://en.wikipedia.org/wiki/Brooks%27s_law) to stay aligned. Nobody established them. Each agent worked on its piece in isolation. The result was code that [mimicked function in form](https://eu.36kr.com/en/p/3643187094507394) but lacked coherent engineering intention, with an [88% CI failure rate](https://www.theregister.com/2026/01/22/cursor_ai_wrote_a_browser/) and not a single clean-compiling commit in the last hundred. Terzian, who maintains Servo, [contrasted](https://pivot-to-ai.com/2026/01/27/cursor-lies-about-vibe-coding-a-web-browser-with-ai/) FastRender's spaghetti with Ladybird's codebase, which he could follow immediately because it tracked the web specifications. The difference was design, not capability. No amount of parallel horsepower can rescue a design that was never made. And the resulting tangle constrains whatever comes next: an AI asked to modify FastRender faces the same cascading breakage a human would. Complexity doesn't care who's struggling with it. If an LLM can always generate more code to manage the complexity it creates, does code quality still matter?

## As the twig is bent, so grows the tree

The most durable output of any design is the thinking embedded in it. Code can be rewritten. The assumptions it carries are harder to replace.

Paul David [documented](https://www.researchgate.net/publication/4724731_The_Dynamo_and_the_Computer_An_Historical_Perspective_On_the_Modern_Productivity_Paradox) a vivid case. Steam-era factories rose several storeys because power came from a single engine turning a [central shaft](https://en.wikipedia.org/wiki/Line_shaft); leather belts and pulleys distributed force to every floor, and machines clustered around the shaft because every foot of belt lost energy. The building was shaped by the engine. When electricity arrived in the 1890s, owners bolted a [dynamo](https://en.wikipedia.org/wiki/Dynamo) where the steam engine had stood and changed nothing else. They kept the multi-storey buildings, the overhead shafts, the belt drives. Productivity barely changed. The gains came only in the 1920s, four decades later, when manufacturers redesigned from scratch for individual motors on individual machines: single-storey plants with floor plans dictated by workflow instead of proximity to a power source. The steam engine was gone. The layout it had demanded persisted for forty years.

Software has its own inherited layouts. In 1965, [Hoare](https://en.wikipedia.org/wiki/Tony_Hoare) added null references to ALGOL W "simply because it was so easy to implement". He later called it his [billion-dollar mistake](https://www.infoq.com/presentations/Null-References-The-Billion-Dollar-Mistake-Tony-Hoare/). The cost comes from propagation: every function that receives a value must check whether it might be null, and every forgotten check is a crash. Languages without null exist, but replacing the implementation was never the hard part. The *idea* that a reference might point to nothing shaped how programmers think about data for sixty years, embedded across the discipline's communication structures long after better alternatives arrived.

<span class="pullquote" text-content="An architectural idea can escape the project it was designed for and echo for generations."></span>A year earlier, [Doug McIlroy](https://en.wikipedia.org/wiki/Douglas_McIlroy) proposed that programs should [connect "like garden hose"](https://en.wikipedia.org/wiki/Pipeline_(Unix)). Ken Thompson implemented them in Unix [overnight](https://www.princeton.edu/~hos/mike/transcripts/thompson.htm), creating the philosophy of small, composable tools. Both were pragmatic decisions by people who couldn't have known their choices would still be shaping the discipline six decades later. Cerf and Kahn took longer over [TCP/IP](https://en.wikipedia.org/wiki/Internet_protocol_suite). They embedded a deliberate philosophy: keep the network simple, push [intelligence to the edges](https://en.wikipedia.org/wiki/End-to-end_principle), let any device that speaks the open protocol participate. The network routes packets; what the endpoints do with them is their business. **An architectural idea can escape the project it was designed for and echo for generations.**

These ideas spread slowly, through teaching and gradual adoption. Hoare's null took sixty years. The next committee works faster.

## Four decades to four weeks

<span class="pullquote left" text-content="The architectural decisions of a single prompting session are already being replicated across the industry."></span>In late 2025, Peter Steinberger [vibe-coded](https://fortune.com/2026/01/31/ai-agent-moltbot-clawdbot-openclaw-data-privacy-security-nightmare-moltbook-social-network/) a WhatsApp relay script in about an hour. It became [OpenClaw](https://github.com/nickelcat/openclaw), which grew to over 200,000 GitHub stars in weeks. Its architecture, a hub-and-spoke WebSocket gateway binding to port 18789 with a custom wire protocol, emerged from a single conversation with an LLM. That architecture is now being [studied as a template](https://blog.agentailor.com/posts/openclaw-architecture-lessons-for-agent-builders) for agent infrastructure, with multiple frameworks replicating its design. [Moltbook](https://fortune.com/2026/02/03/moltbook-ai-social-network-security-researchers-agent-internet/), built on OpenClaw, attracted over a million agent accounts within days before security researchers found [over 135,000 internet-exposed instances](https://www.theregister.com/2026/02/09/openclaw_instances_exposed_vibe_code/) with the default configuration binding to the public internet. The architectural decisions of a single prompting session are already being replicated across the industry.

The exposed instances will likely be secured. The architecture goes deeper. Where TCP/IP keeps the network simple and lets any device participate, OpenClaw inverts the design. Its gateway normalises every platform interaction through channel adapters before it reaches agent logic, concentrating intelligence at the centre rather than distributing it to the edges. A custom wire protocol determines who can compose with whom, replacing McIlroy's universal text streams with proprietary frames. The capability model gives agents broad access to filesystem, credentials and messaging platforms. These aren't security bugs. They're values embedded in the architecture, shaped by the problem OpenClaw was built to solve: centralisation, proprietary interfaces, trust by default.

Frameworks copying this pattern aren't just inheriting a security posture. They're inheriting assumptions about how agents relate to each other and to the systems they touch. Paul David's factory owners needed four decades to shed the steam engine's layout. OpenClaw compressed the same cycle into weeks. By the time the consequences are clear, the architecture will already have shaped the systems built on it and the engineers who build them. **The layout outlasts the engine. Then the layout shapes the next engine.**

Simplicity was never a concession to human limitations. It's what gives the next committee room to change direction. Which of the decisions your agents made today will your successors still be living with?

This question shapes [how we approach AI-assisted engineering at JUXT](/). If you'd like to think through the structure together, [we'd welcome a conversation](mailto:info@juxt.pro?subject=AI-assisted%20engineering).
