<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url(https://themes.googleusercontent.com/fonts/css?kit=XGMkxXUZTA64h2imyzu79g);ul.lst-kix_x3ohw44yawjt-7{list-style-type:none}ul.lst-kix_x3ohw44yawjt-8{list-style-type:none}ul.lst-kix_x3ohw44yawjt-5{list-style-type:none}ul.lst-kix_x3ohw44yawjt-6{list-style-type:none}ul.lst-kix_x3ohw44yawjt-3{list-style-type:none}ul.lst-kix_x3ohw44yawjt-4{list-style-type:none}ul.lst-kix_x3ohw44yawjt-1{list-style-type:none}ul.lst-kix_x3ohw44yawjt-2{list-style-type:none}ul.lst-kix_xtq22fvo1gkw-0{list-style-type:none}ul.lst-kix_x3ohw44yawjt-0{list-style-type:none}ul.lst-kix_xtq22fvo1gkw-1{list-style-type:none}ul.lst-kix_xtq22fvo1gkw-2{list-style-type:none}ul.lst-kix_xtq22fvo1gkw-3{list-style-type:none}ul.lst-kix_xtq22fvo1gkw-4{list-style-type:none}ul.lst-kix_xtq22fvo1gkw-5{list-style-type:none}ul.lst-kix_yvdg1h16k2gd-0{list-style-type:none}ul.lst-kix_xtq22fvo1gkw-6{list-style-type:none}ul.lst-kix_xtq22fvo1gkw-7{list-style-type:none}ul.lst-kix_xtq22fvo1gkw-8{list-style-type:none}.lst-kix_yvdg1h16k2gd-0>li:before{content:"\0025cf   "}.lst-kix_8zuoi17wbidi-2>li:before{content:"\0025a0   "}.lst-kix_8zuoi17wbidi-4>li:before{content:"\0025cb   "}.lst-kix_8zuoi17wbidi-3>li:before{content:"\0025cf   "}.lst-kix_73k2w9726ans-8>li:before{content:"\0025a0   "}.lst-kix_8zuoi17wbidi-6>li:before{content:"\0025cf   "}ul.lst-kix_2y9irgusekoj-8{list-style-type:none}ul.lst-kix_2y9irgusekoj-7{list-style-type:none}.lst-kix_8zuoi17wbidi-5>li:before{content:"\0025a0   "}ul.lst-kix_2y9irgusekoj-6{list-style-type:none}ul.lst-kix_201q2ew56qv-7{list-style-type:none}ul.lst-kix_2y9irgusekoj-5{list-style-type:none}ul.lst-kix_201q2ew56qv-8{list-style-type:none}ul.lst-kix_2y9irgusekoj-4{list-style-type:none}ul.lst-kix_201q2ew56qv-5{list-style-type:none}ul.lst-kix_2y9irgusekoj-3{list-style-type:none}ul.lst-kix_201q2ew56qv-6{list-style-type:none}ul.lst-kix_2y9irgusekoj-2{list-style-type:none}ul.lst-kix_2y9irgusekoj-1{list-style-type:none}ul.lst-kix_2y9irgusekoj-0{list-style-type:none}.lst-kix_8zuoi17wbidi-7>li:before{content:"\0025cb   "}ul.lst-kix_201q2ew56qv-0{list-style-type:none}.lst-kix_8zuoi17wbidi-8>li:before{content:"\0025a0   "}ul.lst-kix_201q2ew56qv-3{list-style-type:none}ul.lst-kix_201q2ew56qv-4{list-style-type:none}ul.lst-kix_201q2ew56qv-1{list-style-type:none}ul.lst-kix_201q2ew56qv-2{list-style-type:none}ul.lst-kix_c96mdw2ek8h6-7{list-style-type:none}ul.lst-kix_c96mdw2ek8h6-8{list-style-type:none}ul.lst-kix_c96mdw2ek8h6-5{list-style-type:none}ul.lst-kix_c96mdw2ek8h6-6{list-style-type:none}ul.lst-kix_c96mdw2ek8h6-3{list-style-type:none}ul.lst-kix_c96mdw2ek8h6-4{list-style-type:none}ul.lst-kix_yvdg1h16k2gd-8{list-style-type:none}ul.lst-kix_yvdg1h16k2gd-7{list-style-type:none}.lst-kix_73k2w9726ans-0>li:before{content:"\0025cf   "}ul.lst-kix_yvdg1h16k2gd-6{list-style-type:none}ul.lst-kix_yvdg1h16k2gd-5{list-style-type:none}ul.lst-kix_yvdg1h16k2gd-4{list-style-type:none}ul.lst-kix_yvdg1h16k2gd-3{list-style-type:none}.lst-kix_73k2w9726ans-1>li:before{content:"\0025cb   "}.lst-kix_73k2w9726ans-2>li:before{content:"\0025a0   "}ul.lst-kix_yvdg1h16k2gd-2{list-style-type:none}ul.lst-kix_yvdg1h16k2gd-1{list-style-type:none}.lst-kix_73k2w9726ans-3>li:before{content:"\0025cf   "}.lst-kix_73k2w9726ans-4>li:before{content:"\0025cb   "}ul.lst-kix_c96mdw2ek8h6-1{list-style-type:none}.lst-kix_73k2w9726ans-7>li:before{content:"\0025cb   "}ul.lst-kix_c96mdw2ek8h6-2{list-style-type:none}.lst-kix_73k2w9726ans-6>li:before{content:"\0025cf   "}ul.lst-kix_c96mdw2ek8h6-0{list-style-type:none}.lst-kix_8zuoi17wbidi-0>li:before{content:"\0025cf   "}.lst-kix_73k2w9726ans-5>li:before{content:"\0025a0   "}.lst-kix_8zuoi17wbidi-1>li:before{content:"\0025cb   "}ul.lst-kix_hoi681h5v2ob-3{list-style-type:none}ul.lst-kix_hoi681h5v2ob-2{list-style-type:none}ul.lst-kix_hoi681h5v2ob-1{list-style-type:none}ul.lst-kix_hoi681h5v2ob-0{list-style-type:none}ul.lst-kix_6ddi85afao4m-7{list-style-type:none}ul.lst-kix_hoi681h5v2ob-7{list-style-type:none}ul.lst-kix_6ddi85afao4m-8{list-style-type:none}ul.lst-kix_hoi681h5v2ob-6{list-style-type:none}ul.lst-kix_6ddi85afao4m-5{list-style-type:none}ul.lst-kix_hoi681h5v2ob-5{list-style-type:none}ul.lst-kix_6ddi85afao4m-6{list-style-type:none}ul.lst-kix_hoi681h5v2ob-4{list-style-type:none}ul.lst-kix_hoi681h5v2ob-8{list-style-type:none}.lst-kix_x3ohw44yawjt-1>li:before{content:"\0025cb   "}.lst-kix_x3ohw44yawjt-3>li:before{content:"\0025cf   "}ul.lst-kix_6ddi85afao4m-3{list-style-type:none}ul.lst-kix_6ddi85afao4m-4{list-style-type:none}ul.lst-kix_6ddi85afao4m-1{list-style-type:none}ul.lst-kix_6ddi85afao4m-2{list-style-type:none}ul.lst-kix_6ddi85afao4m-0{list-style-type:none}.lst-kix_x3ohw44yawjt-5>li:before{content:"\0025a0   "}.lst-kix_x3ohw44yawjt-7>li:before{content:"\0025cb   "}.lst-kix_c96mdw2ek8h6-5>li:before{content:"\0025a0   "}.lst-kix_c96mdw2ek8h6-1>li:before{content:"\0025cb   "}.lst-kix_c96mdw2ek8h6-7>li:before{content:"\0025cb   "}.lst-kix_yvdg1h16k2gd-8>li:before{content:"\0025a0   "}.lst-kix_yvdg1h16k2gd-2>li:before{content:"\0025a0   "}.lst-kix_wclmudotqg4p-0>li:before{content:"\0025cf   "}.lst-kix_yvdg1h16k2gd-6>li:before{content:"\0025cf   "}.lst-kix_wclmudotqg4p-4>li:before{content:"\0025cb   "}.lst-kix_yvdg1h16k2gd-4>li:before{content:"\0025cb   "}.lst-kix_wclmudotqg4p-2>li:before{content:"\0025a0   "}.lst-kix_wclmudotqg4p-8>li:before{content:"\0025a0   "}.lst-kix_fbh6qe311gox-5>li:before{content:"\0025a0   "}.lst-kix_fbh6qe311gox-3>li:before{content:"\0025cf   "}.lst-kix_fbh6qe311gox-7>li:before{content:"\0025cb   "}.lst-kix_fbh6qe311gox-1>li:before{content:"\0025cb   "}.lst-kix_wclmudotqg4p-6>li:before{content:"\0025cf   "}ul.lst-kix_2m8gfn6347xj-7{list-style-type:none}ul.lst-kix_2m8gfn6347xj-8{list-style-type:none}ul.lst-kix_2m8gfn6347xj-5{list-style-type:none}ul.lst-kix_2m8gfn6347xj-6{list-style-type:none}ul.lst-kix_2m8gfn6347xj-3{list-style-type:none}ul.lst-kix_2m8gfn6347xj-4{list-style-type:none}ul.lst-kix_2m8gfn6347xj-1{list-style-type:none}ul.lst-kix_2m8gfn6347xj-2{list-style-type:none}.lst-kix_c96mdw2ek8h6-3>li:before{content:"\0025cf   "}ul.lst-kix_2m8gfn6347xj-0{list-style-type:none}.lst-kix_7kteh8lxs8q-0>li:before{content:"\0025cf   "}.lst-kix_7kteh8lxs8q-3>li:before{content:"\0025cf   "}.lst-kix_5ee0xgh37a3n-8>li:before{content:"\0025a0   "}.lst-kix_2o9nwgwuk4o3-4>li:before{content:"\0025cb   "}.lst-kix_2o9nwgwuk4o3-5>li:before{content:"\0025a0   "}.lst-kix_2m8gfn6347xj-1>li:before{content:"\0025cb   "}.lst-kix_2m8gfn6347xj-2>li:before{content:"\0025a0   "}.lst-kix_2m8gfn6347xj-5>li:before{content:"\0025a0   "}.lst-kix_2m8gfn6347xj-6>li:before{content:"\0025cf   "}.lst-kix_n3vdwnsj00n6-8>li:before{content:"\0025a0   "}.lst-kix_2o9nwgwuk4o3-8>li:before{content:"\0025a0   "}ul.lst-kix_s48yei1wc20v-6{list-style-type:none}.lst-kix_hoi681h5v2ob-8>li:before{content:"\0025a0   "}ul.lst-kix_s48yei1wc20v-7{list-style-type:none}ul.lst-kix_s48yei1wc20v-4{list-style-type:none}ul.lst-kix_s48yei1wc20v-5{list-style-type:none}ul.lst-kix_s48yei1wc20v-2{list-style-type:none}ul.lst-kix_s48yei1wc20v-3{list-style-type:none}ul.lst-kix_s48yei1wc20v-0{list-style-type:none}ul.lst-kix_s48yei1wc20v-1{list-style-type:none}.lst-kix_j8j7ot1jxvac-5>li:before{content:"\0025a0   "}.lst-kix_n3vdwnsj00n6-5>li:before{content:"\0025a0   "}.lst-kix_j8j7ot1jxvac-4>li:before{content:"\0025cb   "}ul.lst-kix_s48yei1wc20v-8{list-style-type:none}.lst-kix_7kteh8lxs8q-8>li:before{content:"\0025a0   "}.lst-kix_n3vdwnsj00n6-4>li:before{content:"\0025cb   "}.lst-kix_7kteh8lxs8q-7>li:before{content:"\0025cb   "}.lst-kix_7kteh8lxs8q-4>li:before{content:"\0025cb   "}.lst-kix_n3vdwnsj00n6-0>li:before{content:"\0025cf   "}.lst-kix_n3vdwnsj00n6-1>li:before{content:"\0025cb   "}.lst-kix_j8j7ot1jxvac-8>li:before{content:"\0025a0   "}ul.lst-kix_x2oe9ytz41j3-7{list-style-type:none}ul.lst-kix_x2oe9ytz41j3-8{list-style-type:none}.lst-kix_uwqspe3pqv2g-5>li:before{content:"\0025a0   "}.lst-kix_uwqspe3pqv2g-6>li:before{content:"\0025cf   "}ul.lst-kix_x2oe9ytz41j3-5{list-style-type:none}ul.lst-kix_x2oe9ytz41j3-6{list-style-type:none}.lst-kix_j8j7ot1jxvac-1>li:before{content:"\0025cb   "}.lst-kix_z5hu9go0lisu-3>li:before{content:"\0025cf   "}.lst-kix_qojftgks3845-0>li:before{content:"\0025cf   "}.lst-kix_j8j7ot1jxvac-0>li:before{content:"\0025cf   "}.lst-kix_z5hu9go0lisu-2>li:before{content:"\0025a0   "}.lst-kix_5ee0xgh37a3n-0>li:before{content:"\0025cf   "}.lst-kix_qojftgks3845-7>li:before{content:"\0025cb   "}.lst-kix_qojftgks3845-4>li:before{content:"\0025cb   "}.lst-kix_qojftgks3845-8>li:before{content:"\0025a0   "}.lst-kix_hoi681h5v2ob-5>li:before{content:"\0025a0   "}.lst-kix_qojftgks3845-3>li:before{content:"\0025cf   "}.lst-kix_hoi681h5v2ob-4>li:before{content:"\0025cb   "}.lst-kix_z5hu9go0lisu-7>li:before{content:"\0025cb   "}.lst-kix_5ee0xgh37a3n-7>li:before{content:"\0025cb   "}.lst-kix_z5hu9go0lisu-6>li:before{content:"\0025cf   "}.lst-kix_hoi681h5v2ob-0>li:before{content:"\0025cf   "}.lst-kix_hoi681h5v2ob-1>li:before{content:"\0025cb   "}.lst-kix_5ee0xgh37a3n-4>li:before{content:"\0025cb   "}ul.lst-kix_x2oe9ytz41j3-0{list-style-type:none}.lst-kix_uwqspe3pqv2g-1>li:before{content:"\0025cb   "}.lst-kix_uwqspe3pqv2g-2>li:before{content:"\0025a0   "}ul.lst-kix_x2oe9ytz41j3-3{list-style-type:none}ul.lst-kix_x2oe9ytz41j3-4{list-style-type:none}ul.lst-kix_x2oe9ytz41j3-1{list-style-type:none}.lst-kix_5ee0xgh37a3n-3>li:before{content:"\0025cf   "}ul.lst-kix_x2oe9ytz41j3-2{list-style-type:none}.lst-kix_dtqeagvh3z61-0>li:before{content:"\0025cf   "}.lst-kix_l4vb4twrdv2l-7>li:before{content:"\0025cb   "}.lst-kix_bcdgnbe0mae0-3>li:before{content:"\0025cf   "}.lst-kix_bcdgnbe0mae0-7>li:before{content:"\0025cb   "}.lst-kix_9qwpiqp3f3by-6>li:before{content:"\0025cf   "}.lst-kix_6ddi85afao4m-2>li:before{content:"\0025a0   "}.lst-kix_6ddi85afao4m-6>li:before{content:"\0025cf   "}.lst-kix_x3ohw44yawjt-0>li:before{content:"\0025cf   "}.lst-kix_x3ohw44yawjt-4>li:before{content:"\0025cb   "}.lst-kix_l4vb4twrdv2l-3>li:before{content:"\0025cf   "}.lst-kix_dtqeagvh3z61-4>li:before{content:"\0025cb   "}.lst-kix_9qwpiqp3f3by-2>li:before{content:"\0025a0   "}.lst-kix_x3ohw44yawjt-8>li:before{content:"\0025a0   "}ul.lst-kix_l4vb4twrdv2l-6{list-style-type:none}ul.lst-kix_l4vb4twrdv2l-7{list-style-type:none}ul.lst-kix_l4vb4twrdv2l-8{list-style-type:none}.lst-kix_dtqeagvh3z61-8>li:before{content:"\0025a0   "}.lst-kix_c96mdw2ek8h6-4>li:before{content:"\0025cb   "}ul.lst-kix_l4vb4twrdv2l-0{list-style-type:none}ul.lst-kix_l4vb4twrdv2l-1{list-style-type:none}ul.lst-kix_l4vb4twrdv2l-2{list-style-type:none}ul.lst-kix_l4vb4twrdv2l-3{list-style-type:none}ul.lst-kix_l4vb4twrdv2l-4{list-style-type:none}ul.lst-kix_l4vb4twrdv2l-5{list-style-type:none}.lst-kix_c96mdw2ek8h6-0>li:before{content:"\0025cf   "}.lst-kix_c96mdw2ek8h6-8>li:before{content:"\0025a0   "}.lst-kix_w2kludwmf31c-6>li:before{content:"\0025cf   "}.lst-kix_yvdg1h16k2gd-1>li:before{content:"\0025cb   "}.lst-kix_wclmudotqg4p-1>li:before{content:"\0025cb   "}.lst-kix_s48yei1wc20v-3>li:before{content:"\0025cf   "}.lst-kix_s48yei1wc20v-7>li:before{content:"\0025cb   "}.lst-kix_yvdg1h16k2gd-5>li:before{content:"\0025a0   "}.lst-kix_97mxapjvt3jm-7>li:before{content:"\0025cb   "}.lst-kix_w2kludwmf31c-2>li:before{content:"\0025a0   "}.lst-kix_fbh6qe311gox-4>li:before{content:"\0025cb   "}.lst-kix_97mxapjvt3jm-3>li:before{content:"\0025cf   "}.lst-kix_fbh6qe311gox-8>li:before{content:"\0025a0   "}.lst-kix_wclmudotqg4p-5>li:before{content:"\0025a0   "}.lst-kix_fbh6qe311gox-0>li:before{content:"\0025cf   "}.lst-kix_2o9nwgwuk4o3-1>li:before{content:"\0025cb   "}ul.lst-kix_htcdhh5wa1du-1{list-style-type:none}ul.lst-kix_htcdhh5wa1du-2{list-style-type:none}ul.lst-kix_htcdhh5wa1du-3{list-style-type:none}ul.lst-kix_htcdhh5wa1du-4{list-style-type:none}ul.lst-kix_htcdhh5wa1du-5{list-style-type:none}ul.lst-kix_htcdhh5wa1du-6{list-style-type:none}ul.lst-kix_htcdhh5wa1du-7{list-style-type:none}ul.lst-kix_htcdhh5wa1du-8{list-style-type:none}.lst-kix_hmgkcvxnm93-1>li:before{content:"\0025cb   "}.lst-kix_hmgkcvxnm93-3>li:before{content:"\0025cf   "}ul.lst-kix_es9fbs6gqhuy-3{list-style-type:none}ul.lst-kix_es9fbs6gqhuy-4{list-style-type:none}.lst-kix_v8gw1emso66v-4>li:before{content:"\0025cb   "}ul.lst-kix_es9fbs6gqhuy-1{list-style-type:none}.lst-kix_hmgkcvxnm93-6>li:before{content:"\0025cf   "}.lst-kix_v8gw1emso66v-5>li:before{content:"\0025a0   "}ul.lst-kix_es9fbs6gqhuy-2{list-style-type:none}.lst-kix_xtq22fvo1gkw-3>li:before{content:"\0025cf   "}ul.lst-kix_es9fbs6gqhuy-7{list-style-type:none}ul.lst-kix_es9fbs6gqhuy-8{list-style-type:none}.lst-kix_v8gw1emso66v-2>li:before{content:"\0025a0   "}ul.lst-kix_es9fbs6gqhuy-5{list-style-type:none}ul.lst-kix_es9fbs6gqhuy-6{list-style-type:none}.lst-kix_xtq22fvo1gkw-2>li:before{content:"\0025a0   "}.lst-kix_hmgkcvxnm93-8>li:before{content:"\0025a0   "}ul.lst-kix_es9fbs6gqhuy-0{list-style-type:none}.lst-kix_xtq22fvo1gkw-0>li:before{content:"\0025cf   "}ul.lst-kix_htcdhh5wa1du-0{list-style-type:none}ul.lst-kix_fjwg50mkqwr6-0{list-style-type:none}ul.lst-kix_fjwg50mkqwr6-1{list-style-type:none}ul.lst-kix_fjwg50mkqwr6-2{list-style-type:none}ul.lst-kix_fjwg50mkqwr6-3{list-style-type:none}ul.lst-kix_fjwg50mkqwr6-4{list-style-type:none}.lst-kix_v8gw1emso66v-7>li:before{content:"\0025cb   "}ul.lst-kix_fjwg50mkqwr6-5{list-style-type:none}ul.lst-kix_fjwg50mkqwr6-6{list-style-type:none}ul.lst-kix_fjwg50mkqwr6-7{list-style-type:none}ul.lst-kix_fjwg50mkqwr6-8{list-style-type:none}.lst-kix_9sh9r41g8bsl-7>li:before{content:"\0025cb   "}.lst-kix_hmgkcvxnm93-0>li:before{content:"\0025cf   "}ul.lst-kix_bcdgnbe0mae0-3{list-style-type:none}ul.lst-kix_bcdgnbe0mae0-4{list-style-type:none}.lst-kix_9sh9r41g8bsl-4>li:before{content:"\0025cb   "}ul.lst-kix_bcdgnbe0mae0-5{list-style-type:none}ul.lst-kix_bcdgnbe0mae0-6{list-style-type:none}ul.lst-kix_bcdgnbe0mae0-0{list-style-type:none}.lst-kix_9sh9r41g8bsl-5>li:before{content:"\0025a0   "}ul.lst-kix_bcdgnbe0mae0-1{list-style-type:none}ul.lst-kix_bcdgnbe0mae0-2{list-style-type:none}ul.lst-kix_bcdgnbe0mae0-7{list-style-type:none}ul.lst-kix_bcdgnbe0mae0-8{list-style-type:none}.lst-kix_9sh9r41g8bsl-2>li:before{content:"\0025a0   "}ul.lst-kix_dtqeagvh3z61-0{list-style-type:none}ul.lst-kix_dtqeagvh3z61-2{list-style-type:none}ul.lst-kix_dtqeagvh3z61-1{list-style-type:none}ul.lst-kix_oxl3rp9hntvg-3{list-style-type:none}ul.lst-kix_oxl3rp9hntvg-4{list-style-type:none}ul.lst-kix_oxl3rp9hntvg-1{list-style-type:none}ul.lst-kix_oxl3rp9hntvg-2{list-style-type:none}ul.lst-kix_oxl3rp9hntvg-0{list-style-type:none}ul.lst-kix_n3vdwnsj00n6-6{list-style-type:none}ul.lst-kix_n3vdwnsj00n6-7{list-style-type:none}ul.lst-kix_n3vdwnsj00n6-4{list-style-type:none}ul.lst-kix_n3vdwnsj00n6-5{list-style-type:none}ul.lst-kix_oxl3rp9hntvg-7{list-style-type:none}ul.lst-kix_n3vdwnsj00n6-2{list-style-type:none}ul.lst-kix_oxl3rp9hntvg-8{list-style-type:none}ul.lst-kix_n3vdwnsj00n6-3{list-style-type:none}ul.lst-kix_oxl3rp9hntvg-5{list-style-type:none}ul.lst-kix_n3vdwnsj00n6-0{list-style-type:none}ul.lst-kix_oxl3rp9hntvg-6{list-style-type:none}ul.lst-kix_n3vdwnsj00n6-1{list-style-type:none}.lst-kix_xtq22fvo1gkw-5>li:before{content:"\0025a0   "}ul.lst-kix_n3vdwnsj00n6-8{list-style-type:none}.lst-kix_xtq22fvo1gkw-8>li:before{content:"\0025a0   "}.lst-kix_ngxftolzt47x-2>li:before{content:"\0025a0   "}.lst-kix_dtqeagvh3z61-3>li:before{content:"\0025cf   "}ul.lst-kix_f1o2lvqu4hpa-3{list-style-type:none}ul.lst-kix_f1o2lvqu4hpa-2{list-style-type:none}ul.lst-kix_f1o2lvqu4hpa-1{list-style-type:none}ul.lst-kix_f1o2lvqu4hpa-0{list-style-type:none}ul.lst-kix_hmgkcvxnm93-3{list-style-type:none}ul.lst-kix_f1o2lvqu4hpa-7{list-style-type:none}ul.lst-kix_hmgkcvxnm93-2{list-style-type:none}ul.lst-kix_f1o2lvqu4hpa-6{list-style-type:none}ul.lst-kix_hmgkcvxnm93-1{list-style-type:none}ul.lst-kix_f1o2lvqu4hpa-5{list-style-type:none}ul.lst-kix_hmgkcvxnm93-0{list-style-type:none}.lst-kix_dtqeagvh3z61-1>li:before{content:"\0025cb   "}ul.lst-kix_f1o2lvqu4hpa-4{list-style-type:none}ul.lst-kix_f1o2lvqu4hpa-8{list-style-type:none}.lst-kix_ngxftolzt47x-8>li:before{content:"\0025a0   "}.lst-kix_bcdgnbe0mae0-4>li:before{content:"\0025cb   "}.lst-kix_bcdgnbe0mae0-6>li:before{content:"\0025cf   "}.lst-kix_9qwpiqp3f3by-7>li:before{content:"\0025cb   "}.lst-kix_6ddi85afao4m-3>li:before{content:"\0025cf   "}.lst-kix_6ddi85afao4m-5>li:before{content:"\0025a0   "}ul.lst-kix_hmgkcvxnm93-7{list-style-type:none}.lst-kix_l4vb4twrdv2l-4>li:before{content:"\0025cb   "}ul.lst-kix_hmgkcvxnm93-6{list-style-type:none}ul.lst-kix_hmgkcvxnm93-5{list-style-type:none}ul.lst-kix_hmgkcvxnm93-4{list-style-type:none}ul.lst-kix_hmgkcvxnm93-8{list-style-type:none}.lst-kix_9qwpiqp3f3by-1>li:before{content:"\0025cb   "}.lst-kix_l4vb4twrdv2l-6>li:before{content:"\0025cf   "}ul.lst-kix_z5hu9go0lisu-8{list-style-type:none}ul.lst-kix_z5hu9go0lisu-7{list-style-type:none}.lst-kix_w2kludwmf31c-3>li:before{content:"\0025cf   "}.lst-kix_w2kludwmf31c-5>li:before{content:"\0025a0   "}ul.lst-kix_z5hu9go0lisu-4{list-style-type:none}ul.lst-kix_2o9nwgwuk4o3-3{list-style-type:none}ul.lst-kix_z5hu9go0lisu-3{list-style-type:none}ul.lst-kix_2o9nwgwuk4o3-2{list-style-type:none}ul.lst-kix_z5hu9go0lisu-6{list-style-type:none}ul.lst-kix_2o9nwgwuk4o3-1{list-style-type:none}ul.lst-kix_z5hu9go0lisu-5{list-style-type:none}ul.lst-kix_2o9nwgwuk4o3-0{list-style-type:none}ul.lst-kix_z5hu9go0lisu-0{list-style-type:none}.lst-kix_fjwg50mkqwr6-3>li:before{content:"\0025cf   "}.lst-kix_fjwg50mkqwr6-5>li:before{content:"\0025a0   "}ul.lst-kix_z5hu9go0lisu-2{list-style-type:none}ul.lst-kix_z5hu9go0lisu-1{list-style-type:none}.lst-kix_97mxapjvt3jm-8>li:before{content:"\0025a0   "}.lst-kix_s48yei1wc20v-4>li:before{content:"\0025cb   "}ul.lst-kix_2o9nwgwuk4o3-7{list-style-type:none}ul.lst-kix_2o9nwgwuk4o3-6{list-style-type:none}ul.lst-kix_2o9nwgwuk4o3-5{list-style-type:none}ul.lst-kix_2o9nwgwuk4o3-4{list-style-type:none}.lst-kix_s48yei1wc20v-6>li:before{content:"\0025cf   "}ul.lst-kix_2o9nwgwuk4o3-8{list-style-type:none}.lst-kix_97mxapjvt3jm-2>li:before{content:"\0025a0   "}.lst-kix_97mxapjvt3jm-0>li:before{content:"\0025cf   "}.lst-kix_doae0emzbhkh-3>li:before{content:"\0025cf   "}.lst-kix_doae0emzbhkh-5>li:before{content:"\0025a0   "}.lst-kix_2o9nwgwuk4o3-2>li:before{content:"\0025a0   "}.lst-kix_ngxftolzt47x-0>li:before{content:"\0025cf   "}ul.lst-kix_wclmudotqg4p-0{list-style-type:none}ul.lst-kix_wclmudotqg4p-1{list-style-type:none}ul.lst-kix_wclmudotqg4p-2{list-style-type:none}ul.lst-kix_wclmudotqg4p-3{list-style-type:none}.lst-kix_x2oe9ytz41j3-2>li:before{content:"\0025a0   "}ul.lst-kix_wclmudotqg4p-4{list-style-type:none}ul.lst-kix_wclmudotqg4p-5{list-style-type:none}ul.lst-kix_wclmudotqg4p-6{list-style-type:none}ul.lst-kix_wclmudotqg4p-7{list-style-type:none}.lst-kix_x2oe9ytz41j3-7>li:before{content:"\0025cb   "}.lst-kix_2m8gfn6347xj-7>li:before{content:"\0025cb   "}ul.lst-kix_wclmudotqg4p-8{list-style-type:none}.lst-kix_7kteh8lxs8q-2>li:before{content:"\0025a0   "}.lst-kix_2m8gfn6347xj-4>li:before{content:"\0025cb   "}.lst-kix_2o9nwgwuk4o3-7>li:before{content:"\0025cb   "}.lst-kix_j8j7ot1jxvac-6>li:before{content:"\0025cf   "}.lst-kix_n3vdwnsj00n6-6>li:before{content:"\0025cf   "}.lst-kix_j8j7ot1jxvac-3>li:before{content:"\0025cf   "}.lst-kix_d96wiom9w4be-3>li:before{content:"\0025cf   "}.lst-kix_n3vdwnsj00n6-3>li:before{content:"\0025cf   "}ul.lst-kix_qojftgks3845-8{list-style-type:none}ul.lst-kix_qojftgks3845-6{list-style-type:none}.lst-kix_hjz7efgetd7f-8>li:before{content:"\0025a0   "}ul.lst-kix_qojftgks3845-7{list-style-type:none}.lst-kix_7kteh8lxs8q-5>li:before{content:"\0025a0   "}.lst-kix_4v5tn5nlxkn0-2>li:before{content:"\0025a0   "}.lst-kix_d96wiom9w4be-0>li:before{content:"\0025cf   "}ul.lst-kix_k4d15ne01hpd-1{list-style-type:none}ul.lst-kix_k4d15ne01hpd-0{list-style-type:none}ul.lst-kix_k4d15ne01hpd-3{list-style-type:none}.lst-kix_hjz7efgetd7f-0>li:before{content:"\0025cf   "}.lst-kix_201q2ew56qv-6>li:before{content:"\0025cf   "}ul.lst-kix_k4d15ne01hpd-2{list-style-type:none}.lst-kix_4tcvq3jpr9zt-5>li:before{content:"\0025a0   "}.lst-kix_uwqspe3pqv2g-4>li:before{content:"\0025cb   "}ul.lst-kix_qojftgks3845-0{list-style-type:none}.lst-kix_hjz7efgetd7f-3>li:before{content:"\0025cf   "}.lst-kix_8stqodlgzzu6-1>li:before{content:"\0025cb   "}ul.lst-kix_qojftgks3845-1{list-style-type:none}ul.lst-kix_k4d15ne01hpd-8{list-style-type:none}ul.lst-kix_qojftgks3845-4{list-style-type:none}ul.lst-kix_k4d15ne01hpd-5{list-style-type:none}ul.lst-kix_qojftgks3845-5{list-style-type:none}ul.lst-kix_k4d15ne01hpd-4{list-style-type:none}.lst-kix_uwqspe3pqv2g-7>li:before{content:"\0025cb   "}ul.lst-kix_qojftgks3845-2{list-style-type:none}ul.lst-kix_k4d15ne01hpd-7{list-style-type:none}.lst-kix_oxl3rp9hntvg-8>li:before{content:"\0025a0   "}.lst-kix_201q2ew56qv-3>li:before{content:"\0025cf   "}ul.lst-kix_qojftgks3845-3{list-style-type:none}ul.lst-kix_k4d15ne01hpd-6{list-style-type:none}.lst-kix_4tcvq3jpr9zt-0>li:before{content:"\0025cf   "}.lst-kix_4tcvq3jpr9zt-8>li:before{content:"\0025a0   "}.lst-kix_4v5tn5nlxkn0-5>li:before{content:"\0025a0   "}.lst-kix_z5hu9go0lisu-1>li:before{content:"\0025cb   "}.lst-kix_z5hu9go0lisu-4>li:before{content:"\0025cb   "}.lst-kix_d96wiom9w4be-8>li:before{content:"\0025a0   "}.lst-kix_hoi681h5v2ob-6>li:before{content:"\0025cf   "}.lst-kix_5ee0xgh37a3n-1>li:before{content:"\0025cb   "}.lst-kix_5ee0xgh37a3n-6>li:before{content:"\0025cf   "}.lst-kix_qojftgks3845-1>li:before{content:"\0025cb   "}.lst-kix_hoi681h5v2ob-3>li:before{content:"\0025cf   "}.lst-kix_oxl3rp9hntvg-0>li:before{content:"\0025cf   "}ul.lst-kix_5mw8rs4kqhy8-8{list-style-type:none}ul.lst-kix_5mw8rs4kqhy8-6{list-style-type:none}.lst-kix_oxl3rp9hntvg-5>li:before{content:"\0025a0   "}ul.lst-kix_5mw8rs4kqhy8-7{list-style-type:none}ul.lst-kix_5mw8rs4kqhy8-4{list-style-type:none}ul.lst-kix_5mw8rs4kqhy8-5{list-style-type:none}ul.lst-kix_5mw8rs4kqhy8-2{list-style-type:none}ul.lst-kix_5mw8rs4kqhy8-3{list-style-type:none}ul.lst-kix_5mw8rs4kqhy8-0{list-style-type:none}ul.lst-kix_5mw8rs4kqhy8-1{list-style-type:none}.lst-kix_qojftgks3845-6>li:before{content:"\0025cf   "}.lst-kix_8stqodlgzzu6-6>li:before{content:"\0025cf   "}ul.lst-kix_hjz7efgetd7f-4{list-style-type:none}ul.lst-kix_hjz7efgetd7f-5{list-style-type:none}ul.lst-kix_hjz7efgetd7f-6{list-style-type:none}ul.lst-kix_hjz7efgetd7f-7{list-style-type:none}ul.lst-kix_hjz7efgetd7f-8{list-style-type:none}.lst-kix_5mw8rs4kqhy8-1>li:before{content:"\0025cb   "}.lst-kix_ngxftolzt47x-5>li:before{content:"\0025a0   "}.lst-kix_l4vb4twrdv2l-1>li:before{content:"\0025cb   "}ul.lst-kix_fbh6qe311gox-3{list-style-type:none}ul.lst-kix_fbh6qe311gox-2{list-style-type:none}ul.lst-kix_fbh6qe311gox-5{list-style-type:none}ul.lst-kix_fbh6qe311gox-4{list-style-type:none}.lst-kix_doae0emzbhkh-0>li:before{content:"\0025cf   "}ul.lst-kix_fbh6qe311gox-1{list-style-type:none}ul.lst-kix_fbh6qe311gox-0{list-style-type:none}.lst-kix_f1o2lvqu4hpa-8>li:before{content:"\0025a0   "}ul.lst-kix_fbh6qe311gox-7{list-style-type:none}ul.lst-kix_fbh6qe311gox-6{list-style-type:none}ul.lst-kix_fbh6qe311gox-8{list-style-type:none}ul.lst-kix_dtqeagvh3z61-8{list-style-type:none}.lst-kix_s48yei1wc20v-1>li:before{content:"\0025cb   "}ul.lst-kix_dtqeagvh3z61-7{list-style-type:none}ul.lst-kix_dtqeagvh3z61-4{list-style-type:none}ul.lst-kix_dtqeagvh3z61-3{list-style-type:none}.lst-kix_6ddi85afao4m-0>li:before{content:"\0025cf   "}ul.lst-kix_dtqeagvh3z61-6{list-style-type:none}.lst-kix_6ddi85afao4m-8>li:before{content:"\0025a0   "}ul.lst-kix_dtqeagvh3z61-5{list-style-type:none}.lst-kix_dtqeagvh3z61-6>li:before{content:"\0025cf   "}.lst-kix_f1o2lvqu4hpa-0>li:before{content:"\0025cf   "}.lst-kix_bcdgnbe0mae0-1>li:before{content:"\0025cb   "}.lst-kix_9qwpiqp3f3by-4>li:before{content:"\0025cb   "}ul.lst-kix_hjz7efgetd7f-0{list-style-type:none}ul.lst-kix_hjz7efgetd7f-1{list-style-type:none}ul.lst-kix_hjz7efgetd7f-2{list-style-type:none}ul.lst-kix_hjz7efgetd7f-3{list-style-type:none}.lst-kix_x3ohw44yawjt-6>li:before{content:"\0025cf   "}.lst-kix_fjwg50mkqwr6-8>li:before{content:"\0025a0   "}.lst-kix_k4d15ne01hpd-6>li:before{content:"\0025cf   "}.lst-kix_es9fbs6gqhuy-2>li:before{content:"\0025a0   "}.lst-kix_yvdg1h16k2gd-7>li:before{content:"\0025cb   "}.lst-kix_w2kludwmf31c-8>li:before{content:"\0025a0   "}.lst-kix_w2kludwmf31c-0>li:before{content:"\0025cf   "}.lst-kix_8f21ae3kep8e-2>li:before{content:"\0025a0   "}.lst-kix_97mxapjvt3jm-5>li:before{content:"\0025a0   "}.lst-kix_htcdhh5wa1du-0>li:before{content:"\0025cf   "}.lst-kix_fbh6qe311gox-2>li:before{content:"\0025a0   "}.lst-kix_wclmudotqg4p-7>li:before{content:"\0025cb   "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_2y9irgusekoj-8>li:before{content:"\0025a0   "}.lst-kix_fjwg50mkqwr6-0>li:before{content:"\0025cf   "}.lst-kix_c96mdw2ek8h6-2>li:before{content:"\0025a0   "}.lst-kix_8k3di5j7b84l-3>li:before{content:"\0025cf   "}.lst-kix_doae0emzbhkh-8>li:before{content:"\0025a0   "}.lst-kix_htcdhh5wa1du-8>li:before{content:"\0025a0   "}.lst-kix_mw2bpz7mwkh-2>li:before{content:"\0025a0   "}ul.lst-kix_w2kludwmf31c-0{list-style-type:none}ul.lst-kix_8k3di5j7b84l-0{list-style-type:none}.lst-kix_8k3di5j7b84l-1>li:before{content:"\0025cb   "}ul.lst-kix_8k3di5j7b84l-1{list-style-type:none}ul.lst-kix_8k3di5j7b84l-2{list-style-type:none}ul.lst-kix_8k3di5j7b84l-3{list-style-type:none}ul.lst-kix_8k3di5j7b84l-4{list-style-type:none}.lst-kix_8k3di5j7b84l-0>li:before{content:"\0025cf   "}ul.lst-kix_8k3di5j7b84l-5{list-style-type:none}.lst-kix_8k3di5j7b84l-5>li:before{content:"\0025a0   "}.lst-kix_8k3di5j7b84l-6>li:before{content:"\0025cf   "}.lst-kix_8k3di5j7b84l-7>li:before{content:"\0025cb   "}.lst-kix_8k3di5j7b84l-8>li:before{content:"\0025a0   "}ul.lst-kix_8k3di5j7b84l-6{list-style-type:none}ul.lst-kix_8k3di5j7b84l-7{list-style-type:none}ul.lst-kix_8k3di5j7b84l-8{list-style-type:none}.lst-kix_2y9irgusekoj-3>li:before{content:"\0025cf   "}.lst-kix_2y9irgusekoj-2>li:before{content:"\0025a0   "}.lst-kix_2y9irgusekoj-1>li:before{content:"\0025cb   "}.lst-kix_2y9irgusekoj-0>li:before{content:"\0025cf   "}ul.lst-kix_w2kludwmf31c-8{list-style-type:none}ul.lst-kix_w2kludwmf31c-7{list-style-type:none}ul.lst-kix_w2kludwmf31c-6{list-style-type:none}ul.lst-kix_w2kludwmf31c-5{list-style-type:none}ul.lst-kix_w2kludwmf31c-4{list-style-type:none}ul.lst-kix_w2kludwmf31c-3{list-style-type:none}ul.lst-kix_w2kludwmf31c-2{list-style-type:none}ul.lst-kix_w2kludwmf31c-1{list-style-type:none}.lst-kix_7emuxpw5bpd9-1>li:before{content:"\0025cb   "}.lst-kix_7emuxpw5bpd9-2>li:before{content:"\0025a0   "}.lst-kix_7emuxpw5bpd9-3>li:before{content:"\0025cf   "}.lst-kix_7emuxpw5bpd9-4>li:before{content:"\0025cb   "}.lst-kix_7emuxpw5bpd9-6>li:before{content:"\0025cf   "}.lst-kix_7emuxpw5bpd9-5>li:before{content:"\0025a0   "}.lst-kix_7emuxpw5bpd9-0>li:before{content:"\0025cf   "}.lst-kix_5mw8rs4kqhy8-0>li:before{content:"\0025cf   "}.lst-kix_5mw8rs4kqhy8-4>li:before{content:"\0025cb   "}.lst-kix_mw2bpz7mwkh-5>li:before{content:"\0025a0   "}.lst-kix_mw2bpz7mwkh-7>li:before{content:"\0025cb   "}.lst-kix_5mw8rs4kqhy8-2>li:before{content:"\0025a0   "}.lst-kix_f1o2lvqu4hpa-7>li:before{content:"\0025cb   "}.lst-kix_f1o2lvqu4hpa-5>li:before{content:"\0025a0   "}.lst-kix_7emuxpw5bpd9-7>li:before{content:"\0025cb   "}.lst-kix_5mw8rs4kqhy8-8>li:before{content:"\0025a0   "}.lst-kix_f1o2lvqu4hpa-1>li:before{content:"\0025cb   "}.lst-kix_f1o2lvqu4hpa-3>li:before{content:"\0025cf   "}.lst-kix_5mw8rs4kqhy8-6>li:before{content:"\0025cf   "}.lst-kix_201q2ew56qv-7>li:before{content:"\0025cb   "}.lst-kix_k4d15ne01hpd-7>li:before{content:"\0025cb   "}.lst-kix_8f21ae3kep8e-7>li:before{content:"\0025cb   "}.lst-kix_es9fbs6gqhuy-1>li:before{content:"\0025cb   "}.lst-kix_8f21ae3kep8e-5>li:before{content:"\0025a0   "}.lst-kix_k4d15ne01hpd-3>li:before{content:"\0025cf   "}.lst-kix_k4d15ne01hpd-5>li:before{content:"\0025a0   "}ul.lst-kix_4v5tn5nlxkn0-8{list-style-type:none}.lst-kix_es9fbs6gqhuy-7>li:before{content:"\0025cb   "}ul.lst-kix_4v5tn5nlxkn0-6{list-style-type:none}ul.lst-kix_4v5tn5nlxkn0-7{list-style-type:none}.lst-kix_8f21ae3kep8e-1>li:before{content:"\0025cb   "}.lst-kix_8f21ae3kep8e-3>li:before{content:"\0025cf   "}.lst-kix_es9fbs6gqhuy-3>li:before{content:"\0025cf   "}.lst-kix_es9fbs6gqhuy-5>li:before{content:"\0025a0   "}.lst-kix_k4d15ne01hpd-1>li:before{content:"\0025cb   "}.lst-kix_htcdhh5wa1du-1>li:before{content:"\0025cb   "}.lst-kix_2y9irgusekoj-5>li:before{content:"\0025a0   "}.lst-kix_htcdhh5wa1du-3>li:before{content:"\0025cf   "}.lst-kix_2y9irgusekoj-7>li:before{content:"\0025cb   "}.lst-kix_htcdhh5wa1du-5>li:before{content:"\0025a0   "}ul.lst-kix_8zuoi17wbidi-0{list-style-type:none}ul.lst-kix_8zuoi17wbidi-1{list-style-type:none}.lst-kix_mw2bpz7mwkh-1>li:before{content:"\0025cb   "}ul.lst-kix_8zuoi17wbidi-2{list-style-type:none}ul.lst-kix_8zuoi17wbidi-3{list-style-type:none}ul.lst-kix_8zuoi17wbidi-4{list-style-type:none}ul.lst-kix_8zuoi17wbidi-5{list-style-type:none}ul.lst-kix_8zuoi17wbidi-6{list-style-type:none}.lst-kix_htcdhh5wa1du-7>li:before{content:"\0025cb   "}ul.lst-kix_8zuoi17wbidi-7{list-style-type:none}ul.lst-kix_8zuoi17wbidi-8{list-style-type:none}.lst-kix_8k3di5j7b84l-4>li:before{content:"\0025cb   "}.lst-kix_mw2bpz7mwkh-3>li:before{content:"\0025cf   "}.lst-kix_8k3di5j7b84l-2>li:before{content:"\0025a0   "}.lst-kix_x2oe9ytz41j3-4>li:before{content:"\0025cb   "}ul.lst-kix_9qwpiqp3f3by-0{list-style-type:none}.lst-kix_x2oe9ytz41j3-1>li:before{content:"\0025cb   "}.lst-kix_x2oe9ytz41j3-5>li:before{content:"\0025a0   "}.lst-kix_x2oe9ytz41j3-0>li:before{content:"\0025cf   "}.lst-kix_x2oe9ytz41j3-8>li:before{content:"\0025a0   "}ul.lst-kix_4v5tn5nlxkn0-0{list-style-type:none}ul.lst-kix_4v5tn5nlxkn0-1{list-style-type:none}.lst-kix_d96wiom9w4be-6>li:before{content:"\0025cf   "}ul.lst-kix_4v5tn5nlxkn0-4{list-style-type:none}.lst-kix_d96wiom9w4be-5>li:before{content:"\0025a0   "}ul.lst-kix_4v5tn5nlxkn0-5{list-style-type:none}ul.lst-kix_4v5tn5nlxkn0-2{list-style-type:none}ul.lst-kix_4v5tn5nlxkn0-3{list-style-type:none}.lst-kix_4v5tn5nlxkn0-4>li:before{content:"\0025cb   "}ul.lst-kix_7kteh8lxs8q-0{list-style-type:none}.lst-kix_d96wiom9w4be-2>li:before{content:"\0025a0   "}.lst-kix_hjz7efgetd7f-6>li:before{content:"\0025cf   "}ul.lst-kix_7kteh8lxs8q-1{list-style-type:none}.lst-kix_4v5tn5nlxkn0-3>li:before{content:"\0025cf   "}ul.lst-kix_7kteh8lxs8q-2{list-style-type:none}ul.lst-kix_7kteh8lxs8q-3{list-style-type:none}ul.lst-kix_7kteh8lxs8q-4{list-style-type:none}ul.lst-kix_7kteh8lxs8q-5{list-style-type:none}ul.lst-kix_7kteh8lxs8q-6{list-style-type:none}ul.lst-kix_9qwpiqp3f3by-4{list-style-type:none}ul.lst-kix_7kteh8lxs8q-7{list-style-type:none}ul.lst-kix_9qwpiqp3f3by-3{list-style-type:none}.lst-kix_4v5tn5nlxkn0-0>li:before{content:"\0025cf   "}ul.lst-kix_7kteh8lxs8q-8{list-style-type:none}ul.lst-kix_9qwpiqp3f3by-2{list-style-type:none}ul.lst-kix_9qwpiqp3f3by-1{list-style-type:none}.lst-kix_d96wiom9w4be-1>li:before{content:"\0025cb   "}ul.lst-kix_9qwpiqp3f3by-8{list-style-type:none}ul.lst-kix_9qwpiqp3f3by-7{list-style-type:none}ul.lst-kix_9qwpiqp3f3by-6{list-style-type:none}ul.lst-kix_9qwpiqp3f3by-5{list-style-type:none}.lst-kix_201q2ew56qv-4>li:before{content:"\0025cb   "}.lst-kix_4tcvq3jpr9zt-3>li:before{content:"\0025cf   "}.lst-kix_8stqodlgzzu6-3>li:before{content:"\0025cf   "}.lst-kix_8stqodlgzzu6-7>li:before{content:"\0025cb   "}ul.lst-kix_ngxftolzt47x-8{list-style-type:none}ul.lst-kix_7emuxpw5bpd9-0{list-style-type:none}.lst-kix_8stqodlgzzu6-4>li:before{content:"\0025cb   "}.lst-kix_8stqodlgzzu6-8>li:before{content:"\0025a0   "}.lst-kix_201q2ew56qv-1>li:before{content:"\0025cb   "}.lst-kix_201q2ew56qv-5>li:before{content:"\0025a0   "}.lst-kix_4tcvq3jpr9zt-2>li:before{content:"\0025a0   "}.lst-kix_4tcvq3jpr9zt-6>li:before{content:"\0025cf   "}ul.lst-kix_7emuxpw5bpd9-1{list-style-type:none}ul.lst-kix_ngxftolzt47x-5{list-style-type:none}ul.lst-kix_ngxftolzt47x-4{list-style-type:none}ul.lst-kix_ngxftolzt47x-7{list-style-type:none}.lst-kix_oxl3rp9hntvg-7>li:before{content:"\0025cb   "}ul.lst-kix_ngxftolzt47x-6{list-style-type:none}.lst-kix_4tcvq3jpr9zt-7>li:before{content:"\0025cb   "}ul.lst-kix_ngxftolzt47x-1{list-style-type:none}.lst-kix_hjz7efgetd7f-5>li:before{content:"\0025a0   "}ul.lst-kix_ngxftolzt47x-0{list-style-type:none}ul.lst-kix_ngxftolzt47x-3{list-style-type:none}ul.lst-kix_ngxftolzt47x-2{list-style-type:none}ul.lst-kix_mw2bpz7mwkh-4{list-style-type:none}ul.lst-kix_mw2bpz7mwkh-3{list-style-type:none}ul.lst-kix_mw2bpz7mwkh-6{list-style-type:none}ul.lst-kix_mw2bpz7mwkh-5{list-style-type:none}.lst-kix_4v5tn5nlxkn0-7>li:before{content:"\0025cb   "}ul.lst-kix_mw2bpz7mwkh-0{list-style-type:none}ul.lst-kix_97mxapjvt3jm-1{list-style-type:none}ul.lst-kix_97mxapjvt3jm-2{list-style-type:none}ul.lst-kix_mw2bpz7mwkh-2{list-style-type:none}.lst-kix_8stqodlgzzu6-0>li:before{content:"\0025cf   "}ul.lst-kix_mw2bpz7mwkh-1{list-style-type:none}ul.lst-kix_97mxapjvt3jm-0{list-style-type:none}ul.lst-kix_7emuxpw5bpd9-2{list-style-type:none}ul.lst-kix_97mxapjvt3jm-5{list-style-type:none}ul.lst-kix_7emuxpw5bpd9-3{list-style-type:none}ul.lst-kix_97mxapjvt3jm-6{list-style-type:none}.lst-kix_4v5tn5nlxkn0-8>li:before{content:"\0025a0   "}ul.lst-kix_7emuxpw5bpd9-4{list-style-type:none}.lst-kix_hjz7efgetd7f-2>li:before{content:"\0025a0   "}ul.lst-kix_97mxapjvt3jm-3{list-style-type:none}.lst-kix_201q2ew56qv-0>li:before{content:"\0025cf   "}ul.lst-kix_7emuxpw5bpd9-5{list-style-type:none}ul.lst-kix_97mxapjvt3jm-4{list-style-type:none}ul.lst-kix_mw2bpz7mwkh-8{list-style-type:none}ul.lst-kix_7emuxpw5bpd9-6{list-style-type:none}.lst-kix_hjz7efgetd7f-1>li:before{content:"\0025cb   "}ul.lst-kix_mw2bpz7mwkh-7{list-style-type:none}ul.lst-kix_7emuxpw5bpd9-7{list-style-type:none}ul.lst-kix_7emuxpw5bpd9-8{list-style-type:none}ul.lst-kix_97mxapjvt3jm-7{list-style-type:none}ul.lst-kix_97mxapjvt3jm-8{list-style-type:none}.lst-kix_oxl3rp9hntvg-3>li:before{content:"\0025cf   "}.lst-kix_oxl3rp9hntvg-2>li:before{content:"\0025a0   "}.lst-kix_oxl3rp9hntvg-6>li:before{content:"\0025cf   "}.lst-kix_5mw8rs4kqhy8-3>li:before{content:"\0025cf   "}.lst-kix_ngxftolzt47x-3>li:before{content:"\0025cf   "}.lst-kix_mw2bpz7mwkh-8>li:before{content:"\0025a0   "}ul.lst-kix_doae0emzbhkh-0{list-style-type:none}ul.lst-kix_doae0emzbhkh-2{list-style-type:none}ul.lst-kix_doae0emzbhkh-1{list-style-type:none}ul.lst-kix_doae0emzbhkh-4{list-style-type:none}ul.lst-kix_doae0emzbhkh-3{list-style-type:none}.lst-kix_doae0emzbhkh-2>li:before{content:"\0025a0   "}ul.lst-kix_doae0emzbhkh-6{list-style-type:none}ul.lst-kix_doae0emzbhkh-5{list-style-type:none}.lst-kix_ngxftolzt47x-7>li:before{content:"\0025cb   "}ul.lst-kix_doae0emzbhkh-8{list-style-type:none}ul.lst-kix_doae0emzbhkh-7{list-style-type:none}.lst-kix_f1o2lvqu4hpa-6>li:before{content:"\0025cf   "}.lst-kix_es9fbs6gqhuy-8>li:before{content:"\0025a0   "}ul.lst-kix_9sh9r41g8bsl-0{list-style-type:none}ul.lst-kix_9sh9r41g8bsl-2{list-style-type:none}ul.lst-kix_9sh9r41g8bsl-1{list-style-type:none}.lst-kix_5mw8rs4kqhy8-7>li:before{content:"\0025cb   "}ul.lst-kix_9sh9r41g8bsl-8{list-style-type:none}ul.lst-kix_9sh9r41g8bsl-7{list-style-type:none}.lst-kix_f1o2lvqu4hpa-2>li:before{content:"\0025a0   "}.lst-kix_201q2ew56qv-8>li:before{content:"\0025a0   "}ul.lst-kix_9sh9r41g8bsl-4{list-style-type:none}ul.lst-kix_9sh9r41g8bsl-3{list-style-type:none}ul.lst-kix_9sh9r41g8bsl-6{list-style-type:none}ul.lst-kix_9sh9r41g8bsl-5{list-style-type:none}.lst-kix_es9fbs6gqhuy-0>li:before{content:"\0025cf   "}.lst-kix_k4d15ne01hpd-8>li:before{content:"\0025a0   "}ul.lst-kix_uwqspe3pqv2g-0{list-style-type:none}.lst-kix_fjwg50mkqwr6-6>li:before{content:"\0025cf   "}.lst-kix_8f21ae3kep8e-4>li:before{content:"\0025cb   "}.lst-kix_8f21ae3kep8e-8>li:before{content:"\0025a0   "}.lst-kix_k4d15ne01hpd-4>li:before{content:"\0025cb   "}.lst-kix_k4d15ne01hpd-0>li:before{content:"\0025cf   "}.lst-kix_8f21ae3kep8e-0>li:before{content:"\0025cf   "}.lst-kix_es9fbs6gqhuy-4>li:before{content:"\0025cb   "}ul.lst-kix_uwqspe3pqv2g-6{list-style-type:none}ul.lst-kix_uwqspe3pqv2g-5{list-style-type:none}ul.lst-kix_uwqspe3pqv2g-8{list-style-type:none}ul.lst-kix_uwqspe3pqv2g-7{list-style-type:none}ul.lst-kix_uwqspe3pqv2g-2{list-style-type:none}ul.lst-kix_uwqspe3pqv2g-1{list-style-type:none}ul.lst-kix_uwqspe3pqv2g-4{list-style-type:none}ul.lst-kix_uwqspe3pqv2g-3{list-style-type:none}.lst-kix_doae0emzbhkh-6>li:before{content:"\0025cf   "}.lst-kix_htcdhh5wa1du-2>li:before{content:"\0025a0   "}.lst-kix_2y9irgusekoj-6>li:before{content:"\0025cf   "}ul.lst-kix_8stqodlgzzu6-2{list-style-type:none}ul.lst-kix_8stqodlgzzu6-3{list-style-type:none}.lst-kix_htcdhh5wa1du-6>li:before{content:"\0025cf   "}ul.lst-kix_8stqodlgzzu6-0{list-style-type:none}ul.lst-kix_8stqodlgzzu6-1{list-style-type:none}ul.lst-kix_8stqodlgzzu6-6{list-style-type:none}ul.lst-kix_8stqodlgzzu6-7{list-style-type:none}ul.lst-kix_8stqodlgzzu6-4{list-style-type:none}ul.lst-kix_8stqodlgzzu6-5{list-style-type:none}ul.lst-kix_v8gw1emso66v-3{list-style-type:none}ul.lst-kix_v8gw1emso66v-2{list-style-type:none}ul.lst-kix_v8gw1emso66v-5{list-style-type:none}ul.lst-kix_8stqodlgzzu6-8{list-style-type:none}ul.lst-kix_v8gw1emso66v-4{list-style-type:none}.lst-kix_fjwg50mkqwr6-2>li:before{content:"\0025a0   "}.lst-kix_mw2bpz7mwkh-0>li:before{content:"\0025cf   "}.lst-kix_mw2bpz7mwkh-4>li:before{content:"\0025cb   "}ul.lst-kix_v8gw1emso66v-1{list-style-type:none}ul.lst-kix_v8gw1emso66v-0{list-style-type:none}ul.lst-kix_v8gw1emso66v-7{list-style-type:none}ul.lst-kix_v8gw1emso66v-6{list-style-type:none}ul.lst-kix_v8gw1emso66v-8{list-style-type:none}.lst-kix_v8gw1emso66v-0>li:before{content:"\0025cf   "}.lst-kix_v8gw1emso66v-1>li:before{content:"\0025cb   "}.lst-kix_hmgkcvxnm93-2>li:before{content:"\0025a0   "}.lst-kix_hmgkcvxnm93-5>li:before{content:"\0025a0   "}.lst-kix_hmgkcvxnm93-4>li:before{content:"\0025cb   "}.lst-kix_v8gw1emso66v-6>li:before{content:"\0025cf   "}.lst-kix_v8gw1emso66v-3>li:before{content:"\0025cf   "}.lst-kix_xtq22fvo1gkw-1>li:before{content:"\0025cb   "}.lst-kix_hmgkcvxnm93-7>li:before{content:"\0025cb   "}.lst-kix_v8gw1emso66v-8>li:before{content:"\0025a0   "}.lst-kix_9sh9r41g8bsl-8>li:before{content:"\0025a0   "}.lst-kix_9sh9r41g8bsl-6>li:before{content:"\0025cf   "}.lst-kix_9sh9r41g8bsl-3>li:before{content:"\0025cf   "}.lst-kix_9sh9r41g8bsl-1>li:before{content:"\0025cb   "}.lst-kix_9sh9r41g8bsl-0>li:before{content:"\0025cf   "}.lst-kix_xtq22fvo1gkw-4>li:before{content:"\0025cb   "}.lst-kix_xtq22fvo1gkw-6>li:before{content:"\0025cf   "}.lst-kix_xtq22fvo1gkw-7>li:before{content:"\0025cb   "}ul.lst-kix_d96wiom9w4be-0{list-style-type:none}ul.lst-kix_d96wiom9w4be-2{list-style-type:none}.lst-kix_ngxftolzt47x-6>li:before{content:"\0025cf   "}ul.lst-kix_d96wiom9w4be-1{list-style-type:none}ul.lst-kix_j8j7ot1jxvac-8{list-style-type:none}ul.lst-kix_d96wiom9w4be-4{list-style-type:none}ul.lst-kix_d96wiom9w4be-3{list-style-type:none}ul.lst-kix_d96wiom9w4be-6{list-style-type:none}.lst-kix_ngxftolzt47x-4>li:before{content:"\0025cb   "}ul.lst-kix_d96wiom9w4be-5{list-style-type:none}ul.lst-kix_d96wiom9w4be-8{list-style-type:none}ul.lst-kix_d96wiom9w4be-7{list-style-type:none}.lst-kix_l4vb4twrdv2l-0>li:before{content:"\0025cf   "}.lst-kix_doae0emzbhkh-1>li:before{content:"\0025cb   "}.lst-kix_l4vb4twrdv2l-8>li:before{content:"\0025a0   "}.lst-kix_s48yei1wc20v-0>li:before{content:"\0025cf   "}.lst-kix_6ddi85afao4m-1>li:before{content:"\0025cb   "}.lst-kix_6ddi85afao4m-7>li:before{content:"\0025cb   "}.lst-kix_l4vb4twrdv2l-2>li:before{content:"\0025a0   "}.lst-kix_s48yei1wc20v-2>li:before{content:"\0025a0   "}ul.lst-kix_j8j7ot1jxvac-2{list-style-type:none}ul.lst-kix_j8j7ot1jxvac-3{list-style-type:none}ul.lst-kix_j8j7ot1jxvac-0{list-style-type:none}ul.lst-kix_j8j7ot1jxvac-1{list-style-type:none}ul.lst-kix_j8j7ot1jxvac-6{list-style-type:none}ul.lst-kix_j8j7ot1jxvac-7{list-style-type:none}ul.lst-kix_j8j7ot1jxvac-4{list-style-type:none}ul.lst-kix_j8j7ot1jxvac-5{list-style-type:none}.lst-kix_dtqeagvh3z61-5>li:before{content:"\0025a0   "}.lst-kix_bcdgnbe0mae0-0>li:before{content:"\0025cf   "}.lst-kix_bcdgnbe0mae0-2>li:before{content:"\0025a0   "}.lst-kix_9qwpiqp3f3by-3>li:before{content:"\0025cf   "}.lst-kix_9qwpiqp3f3by-5>li:before{content:"\0025a0   "}.lst-kix_dtqeagvh3z61-7>li:before{content:"\0025cb   "}.lst-kix_fjwg50mkqwr6-7>li:before{content:"\0025cb   "}.lst-kix_w2kludwmf31c-7>li:before{content:"\0025cb   "}.lst-kix_s48yei1wc20v-8>li:before{content:"\0025a0   "}.lst-kix_97mxapjvt3jm-6>li:before{content:"\0025cf   "}.lst-kix_bcdgnbe0mae0-8>li:before{content:"\0025a0   "}.lst-kix_w2kludwmf31c-1>li:before{content:"\0025cb   "}.lst-kix_97mxapjvt3jm-4>li:before{content:"\0025cb   "}.lst-kix_doae0emzbhkh-7>li:before{content:"\0025cb   "}.lst-kix_fjwg50mkqwr6-1>li:before{content:"\0025cb   "}.lst-kix_2o9nwgwuk4o3-0>li:before{content:"\0025cf   "}.lst-kix_7kteh8lxs8q-1>li:before{content:"\0025cb   "}.lst-kix_x2oe9ytz41j3-3>li:before{content:"\0025cf   "}.lst-kix_x2oe9ytz41j3-6>li:before{content:"\0025cf   "}.lst-kix_2m8gfn6347xj-8>li:before{content:"\0025a0   "}ul.lst-kix_5ee0xgh37a3n-3{list-style-type:none}.lst-kix_2o9nwgwuk4o3-3>li:before{content:"\0025cf   "}ul.lst-kix_5ee0xgh37a3n-4{list-style-type:none}.lst-kix_2m8gfn6347xj-3>li:before{content:"\0025cf   "}ul.lst-kix_5ee0xgh37a3n-5{list-style-type:none}ul.lst-kix_5ee0xgh37a3n-6{list-style-type:none}ul.lst-kix_5ee0xgh37a3n-0{list-style-type:none}ul.lst-kix_5ee0xgh37a3n-1{list-style-type:none}ul.lst-kix_5ee0xgh37a3n-2{list-style-type:none}.lst-kix_n3vdwnsj00n6-7>li:before{content:"\0025cb   "}.lst-kix_2o9nwgwuk4o3-6>li:before{content:"\0025cf   "}.lst-kix_hoi681h5v2ob-7>li:before{content:"\0025cb   "}.lst-kix_d96wiom9w4be-7>li:before{content:"\0025cb   "}.lst-kix_hjz7efgetd7f-7>li:before{content:"\0025cb   "}.lst-kix_d96wiom9w4be-4>li:before{content:"\0025cb   "}.lst-kix_n3vdwnsj00n6-2>li:before{content:"\0025a0   "}.lst-kix_4v5tn5nlxkn0-1>li:before{content:"\0025cb   "}.lst-kix_j8j7ot1jxvac-7>li:before{content:"\0025cb   "}.lst-kix_7kteh8lxs8q-6>li:before{content:"\0025cf   "}.lst-kix_8stqodlgzzu6-5>li:before{content:"\0025a0   "}.lst-kix_uwqspe3pqv2g-8>li:before{content:"\0025a0   "}.lst-kix_hjz7efgetd7f-4>li:before{content:"\0025cb   "}.lst-kix_201q2ew56qv-2>li:before{content:"\0025a0   "}.lst-kix_4tcvq3jpr9zt-1>li:before{content:"\0025cb   "}.lst-kix_8stqodlgzzu6-2>li:before{content:"\0025a0   "}.lst-kix_4v5tn5nlxkn0-6>li:before{content:"\0025cf   "}ul.lst-kix_8f21ae3kep8e-8{list-style-type:none}.lst-kix_j8j7ot1jxvac-2>li:before{content:"\0025a0   "}.lst-kix_z5hu9go0lisu-0>li:before{content:"\0025cf   "}ul.lst-kix_8f21ae3kep8e-2{list-style-type:none}ul.lst-kix_8f21ae3kep8e-3{list-style-type:none}ul.lst-kix_8f21ae3kep8e-0{list-style-type:none}ul.lst-kix_8f21ae3kep8e-1{list-style-type:none}ul.lst-kix_8f21ae3kep8e-6{list-style-type:none}ul.lst-kix_8f21ae3kep8e-7{list-style-type:none}ul.lst-kix_8f21ae3kep8e-4{list-style-type:none}.lst-kix_4tcvq3jpr9zt-4>li:before{content:"\0025cb   "}ul.lst-kix_8f21ae3kep8e-5{list-style-type:none}.lst-kix_z5hu9go0lisu-8>li:before{content:"\0025a0   "}.lst-kix_oxl3rp9hntvg-1>li:before{content:"\0025cb   "}.lst-kix_z5hu9go0lisu-5>li:before{content:"\0025a0   "}ul.lst-kix_5ee0xgh37a3n-7{list-style-type:none}ul.lst-kix_5ee0xgh37a3n-8{list-style-type:none}.lst-kix_qojftgks3845-2>li:before{content:"\0025a0   "}.lst-kix_hoi681h5v2ob-2>li:before{content:"\0025a0   "}.lst-kix_uwqspe3pqv2g-0>li:before{content:"\0025cf   "}.lst-kix_5ee0xgh37a3n-5>li:before{content:"\0025a0   "}.lst-kix_5ee0xgh37a3n-2>li:before{content:"\0025a0   "}.lst-kix_qojftgks3845-5>li:before{content:"\0025a0   "}.lst-kix_uwqspe3pqv2g-3>li:before{content:"\0025cf   "}.lst-kix_oxl3rp9hntvg-4>li:before{content:"\0025cb   "}.lst-kix_dtqeagvh3z61-2>li:before{content:"\0025a0   "}.lst-kix_mw2bpz7mwkh-6>li:before{content:"\0025cf   "}.lst-kix_bcdgnbe0mae0-5>li:before{content:"\0025a0   "}.lst-kix_9qwpiqp3f3by-8>li:before{content:"\0025a0   "}.lst-kix_7emuxpw5bpd9-8>li:before{content:"\0025a0   "}.lst-kix_x3ohw44yawjt-2>li:before{content:"\0025a0   "}ul.lst-kix_4tcvq3jpr9zt-7{list-style-type:none}.lst-kix_9qwpiqp3f3by-0>li:before{content:"\0025cf   "}ul.lst-kix_4tcvq3jpr9zt-8{list-style-type:none}ul.lst-kix_4tcvq3jpr9zt-5{list-style-type:none}ul.lst-kix_4tcvq3jpr9zt-6{list-style-type:none}.lst-kix_l4vb4twrdv2l-5>li:before{content:"\0025a0   "}.lst-kix_f1o2lvqu4hpa-4>li:before{content:"\0025cb   "}.lst-kix_5mw8rs4kqhy8-5>li:before{content:"\0025a0   "}.lst-kix_6ddi85afao4m-4>li:before{content:"\0025cb   "}ul.lst-kix_73k2w9726ans-5{list-style-type:none}ul.lst-kix_73k2w9726ans-6{list-style-type:none}.lst-kix_8f21ae3kep8e-6>li:before{content:"\0025cf   "}.lst-kix_c96mdw2ek8h6-6>li:before{content:"\0025cf   "}ul.lst-kix_73k2w9726ans-7{list-style-type:none}ul.lst-kix_73k2w9726ans-8{list-style-type:none}ul.lst-kix_73k2w9726ans-1{list-style-type:none}ul.lst-kix_73k2w9726ans-2{list-style-type:none}ul.lst-kix_73k2w9726ans-3{list-style-type:none}ul.lst-kix_73k2w9726ans-4{list-style-type:none}.lst-kix_w2kludwmf31c-4>li:before{content:"\0025cb   "}.lst-kix_fjwg50mkqwr6-4>li:before{content:"\0025cb   "}ul.lst-kix_4tcvq3jpr9zt-0{list-style-type:none}ul.lst-kix_4tcvq3jpr9zt-3{list-style-type:none}ul.lst-kix_4tcvq3jpr9zt-4{list-style-type:none}ul.lst-kix_4tcvq3jpr9zt-1{list-style-type:none}ul.lst-kix_4tcvq3jpr9zt-2{list-style-type:none}.lst-kix_s48yei1wc20v-5>li:before{content:"\0025a0   "}.lst-kix_es9fbs6gqhuy-6>li:before{content:"\0025cf   "}.lst-kix_yvdg1h16k2gd-3>li:before{content:"\0025cf   "}.lst-kix_k4d15ne01hpd-2>li:before{content:"\0025a0   "}ul.lst-kix_73k2w9726ans-0{list-style-type:none}.lst-kix_wclmudotqg4p-3>li:before{content:"\0025cf   "}.lst-kix_2y9irgusekoj-4>li:before{content:"\0025cb   "}.lst-kix_97mxapjvt3jm-1>li:before{content:"\0025cb   "}.lst-kix_fbh6qe311gox-6>li:before{content:"\0025cf   "}.lst-kix_2m8gfn6347xj-0>li:before{content:"\0025cf   "}.lst-kix_doae0emzbhkh-4>li:before{content:"\0025cb   "}.lst-kix_htcdhh5wa1du-4>li:before{content:"\0025cb   "}.lst-kix_ngxftolzt47x-1>li:before{content:"\0025cb   "}ol{margin:0;padding:0}table td,table th{padding:0}.c17{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:73.8pt;border-top-color:#000000;border-bottom-style:solid}.c36{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:57.2pt;border-top-color:#000000;border-bottom-style:solid}.c22{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:52pt;border-top-color:#000000;border-bottom-style:solid}.c24{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:52.8pt;border-top-color:#000000;border-bottom-style:solid}.c27{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:75.2pt;border-top-color:#000000;border-bottom-style:solid}.c19{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:38.5pt;border-top-color:#000000;border-bottom-style:solid}.c31{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:72.2pt;border-top-color:#000000;border-bottom-style:solid}.c26{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:64.8pt;border-top-color:#000000;border-bottom-style:solid}.c20{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:66.2pt;border-top-color:#000000;border-bottom-style:solid}.c7{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:60.2pt;border-top-color:#000000;border-bottom-style:solid}.c12{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:49pt;border-top-color:#000000;border-bottom-style:solid}.c1{border-right-style:solid;border-top-width:0pt;border-bottom-color:#000000;border-right-width:0pt;padding-left:0pt;border-left-color:#000000;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;border-top-color:#000000;border-bottom-style:solid;padding-right:0pt}.c9{color:#434343;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c39{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c15{padding-top:16pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c10{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c40{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:26pt;font-family:"Arial";font-style:normal}.c33{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c29{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c25{padding-top:0pt;padding-bottom:16pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c43{color:#666666;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:15pt;font-family:"Arial";font-style:italic}.c41{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Arial";font-style:normal}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c44{padding-top:0pt;padding-bottom:3pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c16{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:center}.c11{padding-top:12pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c5{margin-left:36pt;padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c14{font-weight:400;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c3{margin-left:18pt;padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c32{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c38{border-spacing:0;border-collapse:collapse;margin-right:auto}.c34{padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c8{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c37{background-color:#ffffff;max-width:451.4pt;padding:72pt 72pt 72pt 72pt}.c30{color:#188038;font-weight:400;font-family:"Roboto Mono"}.c13{border:1px solid black;margin:5px}.c18{margin-left:36pt;padding-left:0pt}.c6{color:inherit;text-decoration:inherit}.c42{padding:0;margin:0}.c35{page-break-after:avoid}.c28{height:39.2pt}.c23{height:52.8pt}.c2{height:11pt}.c21{height:26.5pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c37 doc-content"><p class="c35 c44 title" id="h.zcsdgu9oupvj"><span>JUXT AI Radar</span></p><p class="c25 subtitle" id="h.f8dqwlsi6n0x"><span class="c43">An opinionated guide to the AI landscape from JUXT, a Grid Dynamics company</span></p><p class="c0"><span class="c4">Welcome to the first edition of the JUXT AI Radar, where we map the landscape of AI tools, technologies, frameworks, and practices based on our collective expertise and client experiences. Our committee of technology experts has carefully evaluated each entry based on real-world applications, industry trends, and practical utility. This radar represents our current viewpoint and will evolve as the rapidly changing AI ecosystem matures.</span></p><p class="c0 c2"><span class="c4"></span></p><p class="c34"><span class="c14 c8"><a class="c6" href="#h.fbxyq2gzwk4g">Techniques</a></span></p><p class="c3"><span class="c14 c8"><a class="c6" href="#h.tmpexu1mxm5x">Adopt</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.z3hn4xx01uyc">Classical ML</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.kgs7k2yxdj6r">RAG</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.hu9018ffdy1o">LLM-as-a-Judge</a></span></p><p class="c5"><span class="c8 c14"><a class="c6" href="#h.p9mbxbkuqx7q">BERT variants</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.d7b4vah8lrpy">Few-shot prompting</a></span></p><p class="c3"><span class="c14 c8"><a class="c6" href="#h.4frmupuhz2cz">Trial</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.2o75vci0jx5o">Cross-encoder reranking</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.726ff0fbh6eq">Chain of Thought (CoT)</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.oaorhc732eik">Model Distillation &amp; Synthetic data</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.vew319ia715k">UMAP</a></span></p><p class="c3"><span class="c14 c8"><a class="c6" href="#h.cilu8vtefa94">Assess</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.xbx9376o6a">Structured RAG</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.etddact7zixf">Hypothetical Document Embeddings (HyDE)</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.pbuvhggl2spo">Fine-tuning with LoRA</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.v1gmpip9ouph">Agentic tool use</a></span></p><p class="c3"><span class="c14 c8"><a class="c6" href="#h.v6sz99bnmhxk">Hold</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.uhf093qers0x">Word2Vec &amp; GloVe</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.x3e1hyjix13s">t-SNE</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.ht2hw8tzla8l">Zero-shot prompting</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.p6xyyfuksl8w">AI Pull Request Review</a></span></p><p class="c34"><span class="c14 c8"><a class="c6" href="#h.oq75sb72yjms">Languages &amp; Frameworks</a></span></p><p class="c3"><span class="c14 c8"><a class="c6" href="#h.319ath6a9who">Adopt</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.ug2eo2z4rvla">PyTorch</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.qlsckhrlyvie">dbt</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.c05msnbfyu4">Anthropic Model Context Protocol</a></span></p><p class="c3"><span class="c14 c8"><a class="c6" href="#h.6givtpz2b944">Trial</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.u8jyqvdr7yv">AutoGen</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.4a8k4ld5nc3x">DeepEval</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.pmij5wo84n5">LlamaIndex</a></span></p><p class="c3"><span class="c14 c8"><a class="c6" href="#h.ocfxgald0ubc">Assess</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.ce6uw3cgv660">Prolog</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.dc8crxxh29l0">JAX</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.ugsevdqgxfrk">LangChain &amp; LangGraph</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.ytnk67zo7nk">PydanticAI</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.obiqukozi55g">Smolagents</a></span></p><p class="c3"><span class="c14 c8"><a class="c6" href="#h.s3ike813he7k">Hold</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.tn4krrkcqjnr">TensorFlow</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.iwb0ggex554e">Keras</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.tqu5rqjesnoi">R</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.ofqgujays8vl">OpenCL</a></span></p><p class="c34"><span class="c14 c8"><a class="c6" href="#h.dmcxqwupfoqd">Tools</a></span></p><p class="c3"><span class="c14 c8"><a class="c6" href="#h.ugzs2xyc9ine">Adopt</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.7oma38kyuqdb">Software Engineering Copilots</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.l8ey8kh9ieei">Provider-agnostic LLM facades</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.6zjxm26cqll4">Notebooks</a></span></p><p class="c3"><span class="c14 c8"><a class="c6" href="#h.invvqeqjvrp1">Trial</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.z0xxp0axjpep">Agentic Computer Use</a></span></p><p class="c3"><span class="c14 c8"><a class="c6" href="#h.ocybg9tcut5j">Assess</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.r8jmeqirbult">AI Application Bootstrappers</a></span></p><p class="c3"><span class="c14 c8"><a class="c6" href="#h.qlkaf37fyy4h">Hold</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.x1b3f1aayjhj">Conversational data analysis</a></span></p><p class="c34"><span class="c14 c8"><a class="c6" href="#h.h8nbngj6u8tr">Platforms</a></span></p><p class="c3"><span class="c14 c8"><a class="c6" href="#h.5ht4er9vpmx4">Adopt</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.vrsao09mz4xy">Weights &amp; Biases</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.xplxquryxfxx">Foundation models</a></span></p><p class="c3"><span class="c14 c8"><a class="c6" href="#h.uty2xcx8qxs3">Trial</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.ktj5pluzzcbq">MLflow</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.7ywpvb7qyv32">Open weight LLMs</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.nqam9a9mf12r">Lakera</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.w2wfuo9xb4eq">Vector Databases</a></span></p><p class="c3"><span class="c14 c8"><a class="c6" href="#h.lo4rf6hbcuhh">Assess</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.6m9t6zif3fot">Crew.ai</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.hgcbai9fd103">Galileo.ai</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.ixmfl6t1ahoy">Kubeflow</a></span></p><p class="c3"><span class="c14 c8"><a class="c6" href="#h.8ffosw25fu45">Hold</a></span></p><p class="c5"><span class="c14 c8"><a class="c6" href="#h.1trqf7o161xt">Building against vendor-specific APIs</a></span></p><p class="c0 c2"><span class="c4"></span></p><h1 class="c33" id="h.fbxyq2gzwk4g"><span class="c41">Techniques</span></h1><h2 class="c10" id="h.tmpexu1mxm5x"><span class="c29">Adopt</span></h2><h3 class="c15" id="h.z3hn4xx01uyc"><span class="c9">Classical ML</span></h3><p class="c11"><span class="c4">We continue to see tremendous value in classical machine learning approaches like random forests, gradient boosting (XGBoost, LightGBM), linear/logistic regression and support vector machines for many business problems. While attention has shifted dramatically towards deep learning and large language models in the last couple of years, these traditional techniques often provide the best balance of explainability, computational efficiency, and performance for structured data problems.</span></p><p class="c11"><span class="c4">The key advantages that keep classical ML firmly in our Adopt ring include faster training times, lower computing requirements, and easier deployment compared to deep learning approaches. However, it&#39;s important to recognise that realising these benefits requires both quality training data and staff with appropriate expertise. Unlike the recent wave of LLM-based solutions that have democratised AI capabilities for organisations without extensive data science teams, classical ML continues to demand specialised knowledge in feature engineering, model selection, and evaluation.</span></p><p class="c11"><span class="c4">For organisations with the necessary data assets and technical capabilities, these methods work well even with the smaller datasets common in enterprise settings, often matching or exceeding the performance of more complex approaches while remaining more interpretable to stakeholders and easier to maintain. Their lower training costs, smaller carbon footprint, and built-in feature importance metrics provide practical advantages that directly translate to business value, particularly as organisations face increasing pressure to make their ML systems both cost-effective and environmentally sustainable.</span></p><h3 class="c15" id="h.kgs7k2yxdj6r"><span class="c9">RAG</span></h3><p class="c11"><span class="c4">Retrieval-Augmented Generation (RAG) is an AI approach that combines search and text generation to produce more accurate responses. The approach helps prevent confabulation&mdash;cases where AI models generate plausible but incorrect information&mdash;by grounding responses in real data.</span></p><p class="c11"><span class="c4">We&#39;re placing RAG in the Adopt ring because it addresses key challenges in deploying AI systems in information retrieval contexts. The technique is particularly valuable when accuracy and traceability of information are crucial, such as in customer service, technical documentation, or compliance scenarios. While implementing RAG requires careful attention to document processing and embedding strategies, the widespread availability of tools and frameworks has significantly lowered the barriers to adoption. Teams should consider RAG as a foundational technique when building AI applications that need to leverage organisational knowledge.</span></p><p class="c11"><span>We&#39;re particularly interested in monitoring how this technique develops alongside others improving AI system reliability and truthfulness. For example, by augmenting the approach with </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://arxiv.org/abs/2310.11511&amp;sa=D&amp;source=editors&amp;ust=1752765587049330&amp;usg=AOvVaw0_eqAxV1IZI1mD7lMPsSyZ">Self-RAG</a></span><span class="c4">&nbsp;to recognise when more evidence needs to be gathered, conflicting information verified, or responses refined for better accuracy. This &#39;self-criticism&#39; mechanism has shown promising results in improving response quality and reducing hallucinations.</span></p><p class="c11"><span class="c4">See also Cross-encoder reranking, Chain of thought, Structured RAG.</span></p><h3 class="c15" id="h.hu9018ffdy1o"><span class="c9">LLM-as-a-Judge</span></h3><p class="c11"><span class="c4">We&#39;ve placed LLM-as-a-judge in the Adopt ring because it has quickly proven itself to be one of the most practical and cost-effective techniques for evaluating AI system outputs. At first glance, it might seem like circular reasoning to have one LLM evaluate another LLM&#39;s work. However, the capabilities of today&#39;s strongest models are such that they can provide nuanced, multidimensional critique that simpler evaluation methods cannot match, except when using very constrained metrics like exact match or BLEU scores (Bilingual Evaluation Understudy, a method for automatically evaluating machine translations).</span></p><p class="c11"><span class="c4">This technique has become widely adopted in both offline and online evaluation scenarios. In offline evaluation, it scales far better than human assessment, allowing teams to test thousands of outputs quickly during development and quality assurance workflows. In online scenarios, an LLM judge can evaluate another LLM&#39;s output in real-time in production, enabling dynamic workflow adjustments or user experience modifications based on quality assessments. This real-time evaluation approach serves as a foundation for more sophisticated agentic workflows, where multiple AI components collaborate to refine outputs before user delivery.</span></p><p class="c11"><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://arxiv.org/abs/2306.05685&amp;sa=D&amp;source=editors&amp;ust=1752765587053290&amp;usg=AOvVaw12x2b1h2HjAL13SaONufck">Recent research demonstrates</a></span><span class="c4">&nbsp;that the current frontier models can provide judgements that correlate strongly with human preferences across many common evaluation dimensions. For best results, we recommend using a different LLM as the judge than the one being evaluated, and viewing this approach as an augmentation to, not replacement for, human evaluation. The strongest LLMs can identify nuanced issues in reasoning, factuality, and tone that would otherwise require substantial human review time, creating a more efficient evaluation pipeline whilst preserving critical human oversight for final quality assurance.</span></p><h3 class="c11 c35" id="h.p9mbxbkuqx7q"><span>BERT</span><span class="c9">&nbsp;variants</span></h3><p class="c11"><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://huggingface.co/docs/transformers/en/model_doc/bert&amp;sa=D&amp;source=editors&amp;ust=1752765587055103&amp;usg=AOvVaw3U7S5rB76dnLhF7utjXIP_">Bidirectional Encoder Representations from Transformers (BERT)</a></span><span>&nbsp;revolutionised Natural Language Processing (NLP) by allowing AI models to process human language by looking at words in relation to their entire context, rather than just left-to-right or right-to-left. Think of it like a reader who can understand a word by looking at all the surrounding words for context, rather than reading sequentially. The original BERT spawned a family tree of variants and new versions are still being developed, most recently </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://huggingface.co/blog/modernbert&amp;sa=D&amp;source=editors&amp;ust=1752765587056135&amp;usg=AOvVaw0vVXWTW3DtsNfilByciyqT">ModernBERT</a></span><span class="c4">, which is too new at the time of writing for us to have independently evaluated.</span></p><p class="c11"><span class="c4">BERT-style models serve fundamentally different purposes than generative models like GPT. While GPT models excel at generating text and conversational interactions, BERT models are optimised for understanding and analysis tasks such as classification, named entity recognition, and sentiment analysis. They&#39;re particularly valuable for creating semantic vector embeddings that capture text meaning in numerical form, making them essential components in Retrieval Augmented Generation (RAG) systems. In these pipelines, BERT embeddings help retrieve relevant information that is then fed as text to GPT models for generation: the models don&#39;t directly share embeddings, but rather work in complementary roles.</span></p><p class="c11"><span>We particularly recommend </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://huggingface.co/microsoft/deberta-v3-base&amp;sa=D&amp;source=editors&amp;ust=1752765587058306&amp;usg=AOvVaw0Tv1Ch6e5nA5bwxq6E4BRR">DeBERTa</a></span><span>&nbsp;for organisations starting new NLP projects. It handles word relationships more effectively using a disentangled attention mechanism and enhanced position encoding. </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://huggingface.co/docs/transformers/model_doc/distilbert&amp;sa=D&amp;source=editors&amp;ust=1752765587058919&amp;usg=AOvVaw0DOFBamjqpGHvLDGA1qAgS">DistilBERT</a></span><span class="c4">&nbsp;is smaller and faster whilst retaining most of the model&#39;s performance, so it is particularly valuable for production deployments where latency requirements are strict or computing resources are limited, such as edge devices or high-throughput API services.</span></p><p class="c11"><span class="c4">For organisations choosing between BERT and GPT models, consider your specific use case: BERT models require fewer computational resources for inference and excel at precise understanding tasks, while GPT models offer impressive out-of-the-box generation capabilities through accessible APIs. Many sophisticated AI applications today use both types in complementary roles&mdash;BERT for understanding and information retrieval, and GPT for generation based on that understanding.</span></p><p class="c11"><span>There are options for specialised domains like biomedical (</span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://github.com/dmis-lab/biobert&amp;sa=D&amp;source=editors&amp;ust=1752765587061115&amp;usg=AOvVaw1b-MsjZBRSAqeZpSJEKkNd">BioBERT</a></span><span>) or financial text (</span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://huggingface.co/ProsusAI/finbert&amp;sa=D&amp;source=editors&amp;ust=1752765587061326&amp;usg=AOvVaw2WagWxF6CSnwN-zZYmaVdQ">FinBERT</a></span><span class="c4">). While these can outperform general models in their niches, they often require significant expertise to use effectively and may need additional tuning for specific use cases.</span></p><h3 class="c11 c35" id="h.d7b4vah8lrpy"><span class="c9">Few-shot prompting</span></h3><p class="c11"><span class="c4">The technique of providing examples to guide an AI model&#39;s responses has proven consistently effective across different Large Language Models. By showing the model a few examples of desired input-output pairs, developers can achieve more reliable, consistent, and contextually appropriate responses without resorting to complex prompt engineering or fine-tuning.</span></p><p class="c11"><span class="c4">The method&#39;s strength lies in its simplicity and portability across different AI platforms. Our team members report significantly improved results when moving from zero-shot (no examples) to few-shot approaches, particularly for tasks requiring specific formats, technical terminology, or domain expertise. While the optimal number of examples varies by use case, we typically see diminishing returns beyond 3-5 examples. The main trade-off to consider is token consumption, as each example uses up context window space that could be used for other content.</span></p><h2 class="c10" id="h.4frmupuhz2cz"><span class="c29">Trial</span></h2><h3 class="c15" id="h.2o75vci0jx5o"><span class="c9">Cross-encoder reranking</span></h3><p class="c11"><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://sbert.net/examples/applications/retrieve_rerank/README.html&amp;sa=D&amp;source=editors&amp;ust=1752765587064831&amp;usg=AOvVaw0t6g7oHn05fZ2FLsT_yldN">Cross-encoder reranking</a></span><span class="c4">&nbsp;sits in our Trial ring as a promising enhancement for AI search and chat systems. It works alongside traditional embedding-based search (where documents and queries are converted into numbers that represent their meaning) by taking a closer look at the initial search results. While embedding search is fast and good at finding broadly relevant content, cross-encoder reranking excels at understanding subtle relevance signals by looking at the query and potential results together.</span></p><p class="c11"><span class="c4">Most teams we&#39;ve observed use this as a two-step process: first, a quick embedding search finds perhaps 50-100 potentially relevant items from their knowledge base. Then, cross-encoder reranking carefully sorts these candidates to bring the most relevant ones to the top. While this additional step does add some processing time, we&#39;re seeing it deliver meaningful improvements in result quality across various use cases.</span></p><p class="c11"><span>The technique has shown consistent improvements across different domains and use cases, often reducing hallucinations in downstream LLM responses by ensuring higher quality context selection. Implementation has also become more straightforward with libraries like </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://www.sbert.net/&amp;sa=D&amp;source=editors&amp;ust=1752765587068004&amp;usg=AOvVaw2J_1Y3bzPPHw7IVV2P_GVd">sentence-transformers</a></span><span class="c4">&nbsp;providing ready-to-use models. However, teams should be mindful of the additional latency introduced by the reranking step and may need to tune the number of candidates passed to the re-ranker based on their specific performance requirements. The computational overhead is generally justified by the marked improvement in retrieval quality, making this a reliable enhancement to any RAG pipeline where response accuracy is a priority.</span></p><h3 class="c15" id="h.726ff0fbh6eq"><span>Chain of Thought (CoT)</span></h3><p class="c11"><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://learn.microsoft.com/en-us/dotnet/ai/conceptual/chain-of-thought-prompting&amp;sa=D&amp;source=editors&amp;ust=1752765587069629&amp;usg=AOvVaw0jqhGLZayOcf0NWBy-0Np1">Chain of Thought (CoT)</a></span><span class="c4">&nbsp;sits in our Trial ring as a proven technique for improving the reasoning capabilities of large language models, where they are required.</span></p><p class="c11"><span class="c4">This technique involves prompting an AI model to show its step-by-step reasoning process rather than jumping straight to a conclusion. Think of it like asking a student to show their working when solving a problem, rather than just writing down the final answer. Essentially, CoT encourages models to explain their thought process in a structured manner, rather than jumping directly to a conclusion. This has shown to be especially useful in tasks that require complex reasoning, such as mathematical problem-solving or logical inference.</span></p><p class="c11"><span class="c4">We&#39;ve placed CoT in the Trial ring because it has shown promising results in improving the interpretability and accuracy of AI responses when faced with complex tasks. However, it&#39;s worth noting that CoT typically requires more tokens (and thus more cost) than direct prompting, and isn&#39;t always necessary for simple tasks. We recommend using it selectively where the complexity of the task warrants the additional computation and cost. Newer &#39;reasoning&#39; models such as o1 and o3 are specifically built to work with CoT behind the scenes and have very impressive benchmarks at logic/coding tests at the cost of being quite slow and expensive.</span></p><p class="c11"><span>We&#39;re keeping an eye on related techniques such as </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://arxiv.org/abs/2411.05778&amp;sa=D&amp;source=editors&amp;ust=1752765587073440&amp;usg=AOvVaw0LMkhA1YRtZxXEW3fIDEIp">LLMs as Method Actors</a></span><span class="c4">, which achieves similar goals by treating LLMs as actors requiring prompts and cues. However, we caution that this and similar techniques typically require longer, more carefully crafted prompts, which increases token usage and costs. We&#39;re also watching for evidence of whether they consistently outperform simpler prompting approaches in production environments.</span></p><h3 class="c15" id="h.oaorhc732eik"><span>Model d</span><span>istillation</span><span class="c9">&nbsp;&amp; synthetic data</span></h3><p class="c11"><span>We&#39;ve placed </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://platform.openai.com/docs/guides/distillation/model-distillation&amp;sa=D&amp;source=editors&amp;ust=1752765587074957&amp;usg=AOvVaw0-l_wp9bGWLAJDyc9LeIYU">Model Distillation</a></span><span class="c4">&nbsp;in the Trial ring of our Techniques quadrant. Distillation involves training a smaller, more efficient model to mimic a larger one. A common emerging pattern we&#39;re seeing is using LLMs to generate synthetic training data for this smaller model. The larger LLM acts as a &quot;teacher,&quot; creating diverse, high-quality examples that can help the &quot;student&quot; model learn the desired behaviour. For instance, a large model might generate thousands of question-answer pairs that are then used to train a more compact model for a specific domain.</span></p><p class="c11"><span class="c4">This creates an interesting synergy: the large LLM&#39;s ability to generate varied, nuanced responses helps create richer training datasets than might otherwise be available, while distillation makes the resulting solutions more practical to deploy. This approach makes AI deployment more practical and cost-effective, especially for edge devices or resource-constrained environments. However, we&#39;re keeping it in trial as the process still requires considerable expertise to execute well. Teams need to carefully validate the quality of generated training data and ensure the distilled model maintains acceptable performance levels. There&#39;s also ongoing debate about potential amplification of biases or errors through this approach.</span></p><p class="c11"><span>Be sure to check the licence of the model you&#39;re using for distillation. Llama forbids the use of its output to train other models. The launch of DeepSeek R1 in January 2025 brought distillation into popular consciousness, as it has been </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://www.theguardian.com/technology/2025/jan/29/openai-chatgpt-deepseek-china-us-ai-models&amp;sa=D&amp;source=editors&amp;ust=1752765587079213&amp;usg=AOvVaw3J0nIFkkaR_6wuYbq2cLqK">widely assumed that it represents a distillation of existing Foundation models</a></span><span class="c4">.</span></p><h3 class="c11 c35" id="h.vew319ia715k"><span class="c9">UMAP</span></h3><p class="c11"><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://umap-learn.readthedocs.io/en/latest/&amp;sa=D&amp;source=editors&amp;ust=1752765587079787&amp;usg=AOvVaw0gP8-ZnSvM-CLRKz24PJDi">UMAP (Uniform Manifold Approximation and Projection)</a></span><span class="c4">&nbsp;enters our Trial ring as a promising dimensionality reduction technique that&#39;s gaining traction in the AI community. While t-SNE has been the go-to choice for visualising high-dimensional data, UMAP offers better preservation of global structure and runs significantly faster, making it particularly valuable for large-scale AI applications like exploring embedding spaces and analysing neural network activations.</span></p><p class="c11"><span class="c4">We&#39;re seeing successful applications of UMAP across several AI projects, especially in combination with clustering algorithms for understanding large language model behaviours and exploring semantic relationships in vector spaces. However, we recommend starting with smaller, well-understood datasets when first adopting UMAP, as its parameters can be sensitive and require careful tuning to avoid misleading visualisations. The technique shows enough promise and maturity to warrant serious evaluation, though teams should be prepared to invest time in understanding its mathematical foundations to use it effectively.</span></p><p class="c11"><span>The Python </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://umap-learn.readthedocs.io/en/latest/basic_usage.html&amp;sa=D&amp;source=editors&amp;ust=1752765587082599&amp;usg=AOvVaw0xZ73kuqG8dqjfZ2shu-fY">UMAP</a></span><span>&nbsp;library provides extensive documentation and explanation. There are also libraries for </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://github.com/eugenehp/fast-umap&amp;sa=D&amp;source=editors&amp;ust=1752765587082920&amp;usg=AOvVaw2Rd6Rh6HRiITNNZEbK_YxF">Rust</a></span><span>, </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://haifengl.github.io/api/java/smile/manifold/UMAP.html&amp;sa=D&amp;source=editors&amp;ust=1752765587083070&amp;usg=AOvVaw1Px0DoEIKeV2XUDLJ9XD_t">Java</a></span><span>, and </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://cran.r-project.org/web/packages/umap/vignettes/umap.html&amp;sa=D&amp;source=editors&amp;ust=1752765587083219&amp;usg=AOvVaw0mmvHfPFYAHGw7qUKCoFCx">R</a></span><span class="c4">&nbsp;among others.</span></p><h2 class="c10" id="h.cilu8vtefa94"><span class="c29">Assess</span></h2><h3 class="c15" id="h.xbx9376o6a"><span>Structured RAG</span><sup><a href="#cmnt1" id="cmnt_ref1">[a]</a></sup></h3><p class="c1 c0"><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://techcommunity.microsoft.com/blog/azuredevcommunityblog/rag-on-structured-data-with-postgresql/4164456&amp;sa=D&amp;source=editors&amp;ust=1752765587083903&amp;usg=AOvVaw144S7RxSCd4TjN1fDH6GPa">Structured RAG</a></span><span class="c4">&nbsp;extends basic RAG by organising knowledge in a more formal way, rather than just as chunks of text. Think of it like the difference between a filing cabinet (basic RAG) and a well-designed database (structured RAG). Instead of just retrieving text fragments, structured RAG can work with specific fields, relationships, and hierarchies in your data. For example, in a product catalogue, it could separately track and retrieve product names, prices, specifications, and reviews, understanding how these elements relate to each other.</span></p><p class="c1 c0 c2"><span class="c4"></span></p><p class="c1 c0"><span class="c4">The key advantages we&#39;re seeing in real-world applications include more consistent outputs, better handling of complex queries, and reduced confabulation rates compared to traditional RAG approaches. While implementations can vary, successful patterns are emerging around using JSON schemas, XML structures, or database-like organisations for retrieved information.</span></p><p class="c1 c0 c2"><span class="c4"></span></p><p class="c1 c0"><span>However, implementing structured RAG requires more upfront work in data organisation and schema design than traditional RAG. Teams need to carefully consider their data structures and retrieval patterns. This additional complexity is why we&#39;ve placed it in Assess rather than Trial: while the benefits are clear, implementation patterns are still evolving.</span></p><h3 class="c15" id="h.etddact7zixf"><span>Hypothetical Document Embeddings (HyDE)</span></h3><p class="c11"><span>We&#39;ve found </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://arxiv.org/abs/2212.10496&amp;sa=D&amp;source=editors&amp;ust=1752765587087567&amp;usg=AOvVaw0Blwf-nx6D1qgPO2AQBBqX">HyDE (Hypothetical Document Embeddings)</a></span><span class="c4">&nbsp;to be an elegant solution to a common problem in search systems - their tendency to perform poorly when searching content that differs from their training data. HyDE works by first asking a large language model to imagine what an ideal document answering the user&#39;s query might look like. This &#39;hypothetical document&#39; helps bridge the gap between how users naturally ask questions and how information is actually written in documents.</span></p><p class="c11"><span class="c4">The system creates several of these imagined documents (typically five) to capture different ways the answer might be expressed. These are converted into numerical representations (embeddings) and averaged together. This averaged representation is then used to find real documents that are mathematically similar, which often leads to more relevant search results than traditional methods. The approach has proven particularly effective as part of larger systems, such as RAG (Retrieval Augmented Generation), where accurate document retrieval is crucial for generating reliable responses. Teams should evaluate HyDE particularly for cases where high-precision retrieval is crucial and the additional latency is acceptable.</span></p><p class="c11"><span class="c4">See also: RAG, BERT</span></p><h3 class="c15" id="h.pbuvhggl2spo"><span class="c9">Fine-tuning with LoRA</span></h3><p class="c11"><span>We have placed </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://huggingface.co/docs/peft/main/en/conceptual_guides/lora&amp;sa=D&amp;source=editors&amp;ust=1752765587091096&amp;usg=AOvVaw3_BTb5iuLbgeuQZyMCaybQ">Low-Rank Adaptation (LoRA)</a></span><span class="c4">&nbsp;in the Assess ring. LoRA represents a significant advancement in making AI model customisation more practical and cost-effective. Rather than adjusting all parameters in a large language model (which can number in the billions), LoRA adds a small set of trainable parameters while keeping the original model unchanged. Think of it like teaching an expert to adapt to your specific needs without having to retrain their entire knowledge base. This approach typically reduces the computing resources needed for customisation by 3-4 orders of magnitude while maintaining most of the performance benefits of full fine-tuning.</span></p><p class="c11"><span>The technique has proven its value across numerous enterprise applications, and robust tools like Lightning AI&#39;s </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://github.com/Lightning-AI/litgpt/tree/main&amp;sa=D&amp;source=editors&amp;ust=1752765587093154&amp;usg=AOvVaw2zegJC9tBqTCy48pR1nz2Z">lit-gpt</a></span><span>&nbsp;and </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://github.com/axolotl-ai-cloud/axolotl&amp;sa=D&amp;source=editors&amp;ust=1752765587093339&amp;usg=AOvVaw3BD3niYxzUgMyHmPuiMvHZ">axolotl</a></span><span>&nbsp;have emerged to support implementation. However, we place it in the Assess ring rather than Trial because successfully applying LoRA still requires significant machine learning expertise and careful consideration of training data quality. Additionally, we caution organisations to view fine-tuning (including with LoRA) as a short-term investment rather than a long-term strategy. Fine-tuning typically </span><span>ties you to a specific model architecture</span><sup><a href="#cmnt2" id="cmnt_ref2">[b]</a></sup><span class="c4">, and given the rapid pace of AI advancement, tomorrow&#39;s general-purpose models may well outperform your carefully tuned older models with no customisation at all. Migrating fine-tuned weights between different model architectures is particularly challenging and requires a well-curated evaluation corpus. While LoRA is a valuable technique to have in your toolkit, it should only be deployed when the immediate business value clearly outweighs both the technical and opportunity costs.</span></p><h3 class="c11 c35" id="h.v1gmpip9ouph"><span class="c9">Agentic tool use</span></h3><p class="c0 c2"><span class="c4"></span></p><p class="c0"><span class="c4">We&#39;ve placed agentic tool use in the Assess ring. This technique involves Large Language Models using external tools and APIs to augment their capabilities beyond pure language processing.</span></p><p class="c0 c2"><span class="c4"></span></p><p class="c0"><span>The ability of LLMs to use tools represents a significant advancement in AI system architecture. We&#39;re seeing promising applications where LLMs act as orchestrators, calling specialised tools for tasks like data analysis, code execution, or API interactions. However, current implementations often struggle with reliability and can make unpredictable tool choices. While frameworks like </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://www.langchain.com/&amp;sa=D&amp;source=editors&amp;ust=1752765587097666&amp;usg=AOvVaw06xmGFlbMF1Ok15JMus0BV">LangChain</a></span><span>&nbsp;and </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://platform.openai.com/docs/guides/function-calling?api-mode%3Dchat&amp;sa=D&amp;source=editors&amp;ust=1752765587097945&amp;usg=AOvVaw00lbVDc_zAd4SImnRozei0">OpenAI&#39;s Function Calling</a></span><span class="c4">&nbsp;have made tool use more accessible, organisations should carefully evaluate their specific use cases and implement robust validation mechanisms before deploying tool-using LLMs in production environments.</span></p><p class="c0 c2"><span class="c4"></span></p><p class="c0"><span class="c4">The decision to place this in Assess reflects both its potential and current limitations. Early adopters are reporting success with contained, well-defined tool sets, particularly in areas like data analysis and process automation. However, we must emphasise the substantial security risks associated with agentic tool use, especially in environments where malicious actors might attempt to manipulate these systems. It is only a matter of time before poorly secured implementations lead to significant security incidents, with potential for data breaches, unauthorised system access, or service disruption.</span></p><p class="c0 c2"><span class="c4"></span></p><p class="c0"><span>When implementing agentic tool use</span><sup><a href="#cmnt3" id="cmnt_ref3">[c]</a></sup><span class="c4">, several key aspects warrant consideration. Tool selection should be limited to essential, well-tested integrations with comprehensive input validation and output verification in place. Organisations must implement strict access controls, rate limiting, and continuous monitoring of tool usage patterns to detect potential misuse or exploitation attempts. All tool-using agents should operate within sandboxed environments with &lsquo;principle of least privilege&rsquo; enforcement. Security considerations should be paramount in design decisions, with regular penetration testing to identify vulnerabilities before they can be exploited. Additionally, organisations should plan for graceful fallbacks when tools are unavailable or return unexpected results, ensuring system resilience even when tool interactions fail.</span></p><h2 class="c11 c35" id="h.v6sz99bnmhxk"><span class="c29">Hold</span></h2><h3 class="c15" id="h.uhf093qers0x"><span class="c9">Word2Vec &amp; GloVe</span></h3><p class="c11"><span>We&#39;ve placed both </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://nlp.stanford.edu/projects/glove/&amp;sa=D&amp;source=editors&amp;ust=1752765587103004&amp;usg=AOvVaw2uwp03jXj95pKqinwdqdcX">GloVe (Global Vectors for Word Representation)</a></span><span>&nbsp;and </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://www.tensorflow.org/text/tutorials/word2vec&amp;sa=D&amp;source=editors&amp;ust=1752765587103282&amp;usg=AOvVaw1kz44gtMFJpzYlTahptpBd">Word2Vec (Word to Vector)</a></span><span class="c4">&nbsp;in the Hold ring of our techniques quadrant. While these word embedding techniques were groundbreaking when introduced and served as fundamental building blocks for many NLP applications, they have been largely superseded by more advanced approaches.</span></p><p class="c11"><span class="c4">These older embedding techniques, though computationally efficient, lack the contextual understanding that modern transformer-based models provide. Modern large language models and contextual embeddings like BERT produce more nuanced representations that capture word meaning based on surrounding context, rather than the static embeddings that GloVe and Word2Vec generate. For new projects, we recommend exploring more recent embedding techniques (see &quot;BERT Variants&quot; in our Adopt ring) unless you have very specific constraints around computational resources or model size that make these older approaches necessary.</span></p><h3 class="c15" id="h.x3e1hyjix13s"><span class="c9">t-SNE</span></h3><p class="c11"><span>We&rsquo;ve placed </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding&amp;sa=D&amp;source=editors&amp;ust=1752765587106025&amp;usg=AOvVaw1--onT5XwODwVQ9GIORmuz">t-SNE (t-distributed Stochastic Neighbor Embedding)</a></span><span class="c4">&nbsp;in the Hold ring of our techniques quadrant. While t-SNE was groundbreaking when introduced for visualising high-dimensional data in lower dimensions, particularly for understanding the internal representations of neural networks, we&#39;re seeing its limitations become more apparent in modern AI workflows.</span></p><p class="c11"><span class="c4">The core issue is that t-SNE can be misleading when interpreting AI model behaviour, as it prioritises preserving local structure at the expense of global relationships. This can lead teams to draw incorrect conclusions about their models&#39; decision boundaries and feature representations. We&#39;re increasingly recommending alternatives like UMAP (Uniform Manifold Approximation and Projection), which better preserves both local and global structure while offering superior computational performance. For projects requiring dimensionality reduction and visualisation of AI model internals, we suggest exploring these newer techniques rather than defaulting to t-SNE.</span></p><h3 class="c15" id="h.ht2hw8tzla8l"><span class="c9">Zero-shot prompting</span></h3><p class="c11"><span class="c4">Zero-shot prompting &ndash; asking Large Language Models to perform tasks without examples or training &ndash; has been a quick way to get started with AI. However, we strongly recommend against using zero-shot prompts in production without appropriate guardrails and safety measures. We&#39;ve heard of multiple incidents where unprotected prompts led to harmful, biased or inappropriate outputs, potentially exposing organisations to significant risks.</span></p><p class="c11"><span class="c4">Our view is that zero-shot prompting should always be combined with input validation, output filtering and clear usage policies. While it can be valuable for prototyping and exploration, moving to few-shot prompting or fine-tuning with careful guardrails is a more robust approach for production systems. The current placement in &quot;Hold&quot; reflects our concern about organisations rushing to deploy unsafe prompt patterns rather than taking the time to implement proper controls.</span></p><h3 class="c11 c35" id="h.p6xyyfuksl8w"><span class="c9">AI Pull Request Review</span></h3><p class="c11"><span>We&#39;ve placed</span><span>&nbsp;AI Pull Request Review </span><span class="c4">in the Hold ring. Whilst AI tools can catch basic issues like style violations and potential bugs, they fall short in the crucial aspects of PR review that maintain code quality and team effectiveness. The key point is that PR review isn&#39;t just about finding errors: it&#39;s a vital knowledge-sharing mechanism where senior developers mentor juniors, architectural decisions are questioned and refined, and the team maintains a shared understanding of the codebase.</span></p><p class="c11"><span class="c4">Based on our observations across multiple teams, AI review tools tend to focus on surface-level feedback while missing deeper architectural issues, implementation trade-offs, and business logic errors that human reviewers catch. More concerning is that teams who rely heavily on AI reviews often see a decline in collective code ownership and technical knowledge sharing.</span></p><p class="c11"><span class="c4">The recent explosion of AI coding assistants has revealed that whilst they are sometimes helpful for tasks like code completion and refactoring, they struggle with higher-level software engineering decisions that require deep context and experience. As one tech lead noted in our research, &quot;AI can tell you if your code follows patterns, but it can&#39;t tell you if you&#39;re using the right patterns in the first place.&quot; Until AI systems can better understand architectural implications and business context, we recommend maintaining human-driven code reviews as a core practice.</span></p><h1 class="c33" id="h.oq75sb72yjms"><span class="c41">Languages &amp; Frameworks</span></h1><h2 class="c10" id="h.319ath6a9who"><span>Adopt</span></h2><h3 class="c15" id="h.ug2eo2z4rvla"><span class="c9">PyTorch</span></h3><p class="c0"><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://pytorch.org/&amp;sa=D&amp;source=editors&amp;ust=1752765587115588&amp;usg=AOvVaw0f5O6Y-4aWDv0Oyfa29T_g">PyTorch</a></span><span class="c4">&nbsp;has demonstrated consistent maturity and widespread adoption across both research and production environments, earning its place in our Adopt ring. We&#39;re seeing it emerge as the default choice for many machine learning teams, particularly those working on deep learning projects, thanks to its intuitive Python-first approach and dynamic computational graphs that make debugging and prototyping significantly easier.</span></p><p class="c0 c2"><span class="c4"></span></p><p class="c1 c0"><span class="c4">The framework&#39;s robust ecosystem, exceptional documentation and strong community support make it a reliable choice for teams at any scale. While TensorFlow remains relevant, particularly in production deployments, PyTorch&#39;s seamless integration with popular machine learning tools, extensive pre-trained model repository and growing deployment options through TorchServe have addressed previous concerns about production readiness. The framework&#39;s adoption by major technology organisations and research institutions, coupled with its regular release cycle and stability, gives us confidence in recommending it as a default choice for new machine learning projects.</span></p><h3 class="c15" id="h.qlsckhrlyvie"><span class="c9">dbt</span></h3><p class="c0"><span>We&#39;ve placed </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://www.getdbt.com/&amp;sa=D&amp;source=editors&amp;ust=1752765587118812&amp;usg=AOvVaw0EQLclSEdZj_YicvneOufI">dbt (data build tool)</a></span><span class="c4">&nbsp;in the Adopt ring because it has proven to be an essential framework for organising and managing the data transformations that feed AI systems. dbt brings software engineering best practices like version control, testing, and documentation to data transformation workflows, which is crucial when preparing data for AI model training and inference.</span></p><p class="c11"><span class="c4">The reliability and maintainability of AI systems heavily depend on the quality of their input data, and dbt helps teams achieve this by making data transformations more transparent and trustworthy. We&#39;ve seen teams successfully use dbt to create clean, well-documented data pipelines that connect data warehouses to AI applications, while maintaining the agility to quickly adapt to changing requirements. Its integration with modern data platforms and strong community support make it a solid choice for organisations building out their AI infrastructure.</span></p><h3 class="c15" id="h.c05msnbfyu4"><span class="c9">Anthropic Model Context Protocol</span></h3><p class="c0"><span>We&#39;ve placed Anthropic&#39;s </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://docs.anthropic.com/en/docs/agents-and-tools/mcp&amp;sa=D&amp;source=editors&amp;ust=1752765587121683&amp;usg=AOvVaw28iphD9UzdjFzldkfPvR-d">Model Context Protocol (MCP)</a></span><span class="c4">&nbsp;in the Adopt ring because it addresses a critical challenge in AI applications: the need for standardised integration between language models and external tools.</span></p><p class="c11"><span class="c4">The Model Context Protocol provides a well-designed, consistent interface that allows developers to connect LLMs to various tools like databases, search engines, and data sources without having to reinvent integration patterns for each one. Based on our team&#39;s experience, this significantly reduces development time while improving reliability. The protocol&#39;s growing ecosystem of third-party tool integrations means developers can implement complex AI agent capabilities with minimal custom code, focusing instead on their application&#39;s unique value.</span></p><p class="c11"><span class="c4">We&#39;re particularly impressed by how the protocol handles context management and tool discovery, helping models effectively reason about when and how to use available capabilities. Companies deploying AI assistants that need to interact with company data or perform specialised actions should seriously consider adopting this standard rather than building custom integration layers that will likely be more fragile and require more maintenance.</span></p><h2 class="c10" id="h.6givtpz2b944"><span class="c29">Trial</span></h2><h3 class="c15" id="h.u8jyqvdr7yv"><span class="c9">AutoGen</span></h3><p class="c0"><span>We&#39;ve placed </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://microsoft.github.io/autogen/stable//index.html&amp;sa=D&amp;source=editors&amp;ust=1752765587125166&amp;usg=AOvVaw3eby-sXFvGmVqoUiDPmdWv">AutoGen</a></span><span class="c4">&nbsp;in the Trial ring based on its promising approach to orchestrating multiple AI agents for complex problem-solving. This Microsoft-developed framework enables developers to create systems where AI agents can collaborate, dividing tasks between specialised roles like coding, testing, and reviewing, similar to how human development teams operate. While still evolving, we&#39;ve seen compelling early results from teams using AutoGen to build more sophisticated AI applications, particularly in scenarios requiring multi-step reasoning or specialised domain knowledge.</span></p><p class="c0 c2"><span class="c4"></span></p><p class="c0"><span class="c4">The framework&#39;s ability to handle interaction patterns between agents with built-in error handling and recovery shows particular promise for enterprise applications. However, we recommend carefully evaluating its fit for your specific use case, as the overhead of managing multiple agents may not be justified for simpler applications where a single large language model would suffice. We&#39;re also watching how the framework&#39;s approach to agent coordination evolves as the field matures.</span></p><h3 class="c15" id="h.4a8k4ld5nc3x"><span class="c9">DeepEval</span></h3><p class="c0"><span>We&#39;ve placed </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://github.com/confident-ai/deepeval&amp;sa=D&amp;source=editors&amp;ust=1752765587128302&amp;usg=AOvVaw38bWC3eiHNzhe6wIXVgGaw">DeepEval</a></span><span class="c4">&nbsp;in the Trial ring as it addresses a critical gap in AI application development: the systematic evaluation of Large Language Model outputs. While traditional software testing frameworks focus on deterministic outcomes, DeepEval provides a comprehensive toolkit for assessing the reliability, accuracy and consistency of AI-generated content.</span></p><p class="c0 c2"><span class="c4"></span></p><p class="c0"><span>The framework stands out for its practical approach to testing LLM applications, offering built-in metrics for evaluating responses across dimensions like relevance, toxicity and factual accuracy. What particularly impressed our committee was its ability to handle both unit and integration testing scenarios, making it valuable for teams building production-grade AI systems. However, we recommend starting with smaller, non-critical components first, as best practices around LLM testing are still emerging and the framework itself is relatively new to the ecosystem.</span></p><h3 class="c15" id="h.pmij5wo84n5"><span class="c9">LlamaIndex</span></h3><p class="c0"><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://www.llamaindex.ai/&amp;sa=D&amp;source=editors&amp;ust=1752765587131075&amp;usg=AOvVaw1BCrsaBZZdI6h7bjtdrrjH">LlamaIndex</a></span><span class="c4">, formerly known as GPT Index, is a framework that supports developers in connecting large language models with external data sources in a structured way. It provides tools to build indices&mdash;data structures that help LLMs access relevant information efficiently&mdash;thereby improving their ability to handle specific tasks requiring contextual or domain-specific data.</span></p><p class="c0 c2"><span class="c4"></span></p><p class="c0"><span class="c4">We consider LlamaIndex suitable for teams trialling methods to augment LLM performance, especially in data-centric applications. While its modular design and focus on customisation are appealing, its relative maturity as a toolkit means that teams may encounter challenges around documentation, setup, or adapting it to complex datasets. As with many emerging tools, its value depends on careful experimentation and matching it to the right problem space.</span></p><h2 class="c10" id="h.ocfxgald0ubc"><span class="c29">Assess</span></h2><h3 class="c15" id="h.ce6uw3cgv660"><span>Prolog</span></h3><p class="c0"><span>We&#39;ve placed </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=http://www.gprolog.org/&amp;sa=D&amp;source=editors&amp;ust=1752765587133787&amp;usg=AOvVaw2Ns6Zyj9PyAviWKA--KnjW">Prolog</a></span><span class="c4">&nbsp;in the Assess ring of our languages quadrant due to its renewed relevance in AI development, particularly for adding structured logical reasoning capabilities to Large Language Model applications, and decoupling logic from procedure. Prolog (and logic programming in general) may offer significant value due to its ability to extract from and represent knowledge graphs, which have a well-studied symbiotic relationship with LLMs, allowing us to couple the versatility of LLMs with the ability to have a concrete expert knowledge base to prevent hallucinations, reify concrete rules, etc. This also can allow LLMs to produce consumable data for further engineering needs, and allows us to express preferences in our systems in unambiguous ways. The use of such expert systems alongside LLMs has been likened to Kahneman&#39;s system 1 and 2. Finally, the metaprogramming &amp; dynamic capabilities of Prolog are extremely strong.</span></p><p class="c0 c2"><span class="c4"></span></p><p class="c0"><span class="c4">While Prolog has been around since the 1970s, we&#39;re seeing interesting experiments where developers combine its powerful symbolic reasoning with modern LLMs to create more robust and explainable AI systems, by leveraging Prolog as a reasoning agent. However there are challenges around performance, as well as some redundancy in knowledge graphs given the existence of semantic web languages such as RDF, OWL, SPARQL, etc. Prolog is also not the only language of its kind&ndash; there are many kinds of logic language, which are all fundamentally different from each other (E.G., some are used for induction as in SATs, some don&rsquo;t use the same kinds of logic), though this does not necessarily discount Prolog&rsquo;s utility. Since Prolog interoperates extremely well with most other programming languages, it can also be embedded within applications rather easily.</span></p><p class="c0 c2"><span class="c4"></span></p><p class="c0"><span class="c4">The renewed interest doesn&#39;t yet warrant a higher ring placement, as adoption patterns are still emerging and the tooling ecosystem needs maturation. However, we believe technical teams should assess Prolog&#39;s potential, especially for projects where transparent logical reasoning needs to be combined with LLM capabilities. Teams working on applications in regulated industries or those requiring auditable decision paths may find particular value in exploring this approach. At the very least, surveying Prolog provides insight into the possibilities of where historical findings might enrich the current space.</span></p><h3 class="c15" id="h.dc8crxxh29l0"><span class="c9">JAX</span></h3><p class="c0"><span>We&rsquo;ve placed </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://github.com/jax-ml/jax&amp;sa=D&amp;source=editors&amp;ust=1752765587140224&amp;usg=AOvVaw007m9pOPcC2vW_kPOs92mQ">JAX</a></span><span class="c4">&nbsp;in our Assess ring as we observe increasing interest in this ML framework that combines NumPy&#39;s familiar API with hardware acceleration and automatic differentiation. While TensorFlow and PyTorch remain dominant in the ML ecosystem, we&#39;re seeing JAX gain traction particularly in research settings and among teams working on custom ML architectures.</span></p><p class="c0 c2"><span class="c4"></span></p><p class="c0"><span class="c4">What interests us about JAX is its functional approach to ML computation and its ability to compile to multiple hardware targets through XLA (Accelerated Linear Algebra). The framework shows promise for projects requiring high-performance numerical computing, though we suggest careful evaluation of its relative immaturity in areas like deployment tooling and the smaller ecosystem of pre-built components compared to more established frameworks. We recommend teams experimenting with JAX do so on research projects or contained proofs-of-concept before considering broader adoption.</span></p><h3 class="c15" id="h.ugsevdqgxfrk"><span class="c9">LangChain &amp; LangGraph</span></h3><p class="c0"><span>We&#39;ve placed </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://www.langchain.com/&amp;sa=D&amp;source=editors&amp;ust=1752765587142688&amp;usg=AOvVaw0FRaXfnR0_io2SSerwzLqW">LangChain</a></span><span>&nbsp;and its companion </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://www.langchain.com/langgraph&amp;sa=D&amp;source=editors&amp;ust=1752765587142942&amp;usg=AOvVaw3krvaJTYQI5tRBhnLfJ_Yh">LangGraph</a></span><span class="c4">&nbsp;in the Assess ring as they represent an emerging approach to building applications with Large Language Models. These frameworks provide structured ways to compose AI capabilities into more complex applications, with LangChain focusing on general-purpose AI interactions and LangGraph extending this to handle more sophisticated multi-step processes.</span></p><p class="c0 c2"><span class="c4"></span></p><p class="c0"><span class="c4">While these tools have gained significant adoption and show promise in reducing boilerplate code when working with LLMs, we recommend careful evaluation before widespread use. The rapid pace of change in the underlying AI platforms means that some of LangChain&#39;s abstractions may become outdated or less relevant as the ecosystem evolves. We&#39;ve observed teams successfully using these frameworks for prototypes and smaller production systems, but also encountering challenges when requirements grow more complex or when they need to debug unexpected behaviours. Consider starting with focused experiments that test whether these tools truly simplify your specific use case rather than assuming they&#39;re the right choice for all AI development.</span></p><h3 class="c15" id="h.ytnk67zo7nk"><span class="c9">PydanticAI</span></h3><p class="c0"><span>We&#39;ve placed </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://ai.pydantic.dev/&amp;sa=D&amp;source=editors&amp;ust=1752765587145824&amp;usg=AOvVaw2E4Hh6n4BEex-gJupSFUiB">PydanticAI</a></span><span class="c4">&nbsp;in the Assess ring of our Languages &amp; Frameworks quadrant because it represents a promising approach to building AI applications that merits closer examination, while not yet being broadly proven in production environments.</span></p><p class="c0 c2"><span class="c4"></span></p><p class="c0"><span class="c4">PydanticAI brings the well-regarded developer experience of FastAPI to generative AI application development. Built by the team behind Pydantic (which has become a foundation for many AI frameworks including OpenAI SDK, Anthropic SDK, LangChain, and others), it offers a familiar, Python-centric approach to building LLM-powered applications. The framework provides important features like model-agnostic support across major LLM providers, structured responses through Pydantic validation, and a dependency injection system that facilitates testing.</span></p><p class="c0 c2"><span class="c4"></span></p><p class="c0"><span class="c4">What particularly interests us is how PydanticAI leverages existing Python patterns and best practices rather than introducing completely new paradigms. This could significantly lower the learning curve for developers working with AI. However, as a relatively new framework in a rapidly evolving space, we&#39;re placing it in Assess while we watch for broader adoption, community growth, and production-proven implementations across different use cases. Organisations with Python-based stacks and teams familiar with FastAPI or Pydantic should consider evaluating PydanticAI for their AI application development needs.</span></p><h3 class="c15" id="h.obiqukozi55g"><span class="c9">Smolagents</span></h3><p class="c0"><span>We&#39;ve placed </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://github.com/huggingface/smolagents&amp;sa=D&amp;source=editors&amp;ust=1752765587149455&amp;usg=AOvVaw3URNdfKQodIyUdCWSeMQYN">smolagents</a></span><span class="c4">&nbsp;in the Assess ring of the Languages &amp; Frameworks quadrant based on our evaluation of its current state and potential.</span></p><p class="c0 c2"><span class="c4"></span></p><p class="c0"><span>This lightweight agent framework takes a minimalist approach with its core codebase of under 1,000 lines. Early feedback suggests it can be effective for quickly prototyping agentic concepts before transitioning to more robust frameworks like </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://opensource.microsoft.com/&amp;sa=D&amp;source=editors&amp;ust=1752765587150635&amp;usg=AOvVaw1SnvVirw8xjSTu5b_lFuk0">AutoGen</a></span><span>&nbsp;or </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://www.langchain.com/langgraph&amp;sa=D&amp;source=editors&amp;ust=1752765587150798&amp;usg=AOvVaw2Facj55W75aXz3-on_qtwK">LangGraph</a></span><span class="c4">&nbsp;for production implementations. The framework&#39;s code-based agent approach, where agents execute actions as Python code snippets, appears to reduce the number of steps and LLM calls in certain scenarios, though this comes with inherent security considerations.</span></p><p class="c0 c2"><span class="c4"></span></p><p class="c0"><span class="c4">We&#39;ve positioned smolagents in Assess rather than Trial for several reasons: it lacks extensive production validation, the security implications of code execution require careful evaluation, and while benchmark results with models like DeepSeek-R1 are interesting, we need to see more diverse real-world implementations. Teams exploring agent architectures should evaluate whether SmolaGents&#39; approach aligns with their specific needs and security requirements, whilst recognising its limitations for production-grade systems.</span></p><h2 class="c10" id="h.s3ike813he7k"><span class="c29">Hold</span></h2><h3 class="c15" id="h.tn4krrkcqjnr"><span class="c9">TensorFlow</span></h3><p class="c0"><span>We have placed </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://www.tensorflow.org/&amp;sa=D&amp;source=editors&amp;ust=1752765587153301&amp;usg=AOvVaw0M8zuLvT8cSwuWP5HGwKKn">TensorFlow</a></span><span class="c4">&nbsp;in the Hold ring for several reasons. While TensorFlow remains a capable deep learning framework that helped popularise machine learning at scale, we&#39;re seeing teams struggle with its steep learning curve and complex deployment story compared to more modern alternatives. The framework&#39;s verbose syntax and intricate architecture often lead to longer development cycles, particularly for teams new to machine learning.</span></p><p class="c0 c2"><span class="c4"></span></p><p class="c0"><span class="c4">PyTorch has emerged as the clear community favourite for both research and production deployments, with a more intuitive programming model and better debugging capabilities. Additionally, with the rise of AI platforms that abstract away much of the underlying complexity, many teams no longer need to work directly with low-level frameworks like TensorFlow. For new projects, we recommend exploring higher-level tools or PyTorch unless there are compelling reasons to use TensorFlow, such as maintaining existing deployments or specific requirements around TensorFlow Extended (TFX) for ML pipelines.</span></p><h3 class="c15" id="h.iwb0ggex554e"><span class="c9">Keras</span></h3><p class="c0"><span>We have placed </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://keras.io/&amp;sa=D&amp;source=editors&amp;ust=1752765587156241&amp;usg=AOvVaw25_iV83Hzsq0LaBaOyuWki">Keras</a></span><span class="c4">&nbsp;in the Hold ring primarily due to its transition from a standalone deep learning framework to becoming more tightly integrated with TensorFlow, along with the emergence of more modern alternatives that offer better developer experiences.</span></p><p class="c0 c2"><span class="c4"></span></p><p class="c0"><span class="c4">While Keras served as an excellent entry point for many developers into deep learning, providing an intuitive API that made neural networks more accessible, the landscape has evolved significantly. Frameworks like PyTorch have gained substantial momentum, offering clearer debugging, better documentation and a more Pythonic approach. Additionally, recent high-level frameworks such as Lightning and FastAI provide similar ease-of-use benefits while maintaining closer alignment with current best practices in deep learning development. For new projects, we recommend exploring these alternatives rather than investing in Keras-specific expertise.</span></p><h3 class="c15" id="h.tqu5rqjesnoi"><span class="c9">R</span></h3><p class="c0"><span>Despite </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://www.r-project.org/&amp;sa=D&amp;source=editors&amp;ust=1752765587158836&amp;usg=AOvVaw3iyeiG3UotgS8LI34rbKOG">R</a></span><span class="c4">&#39;s historical significance in data science and statistical computing, we&#39;ve placed it in the Hold ring for new projects. While R remains capable for statistical analysis and data visualisation, we&#39;re seeing its adoption declining in favour of Python&#39;s more comprehensive ecosystem for machine learning and AI workflows.</span></p><p class="c0 c2"><span class="c4"></span></p><p class="c0"><span class="c4">The key factors driving this recommendation are the overwhelming industry preference for Python-based ML frameworks, the stronger integration of Python with modern AI platforms and tools, and the challenges of hiring R specialists in today&#39;s market. While R retains some advantages for specific statistical applications and academic research, we believe teams starting new AI initiatives will benefit from standardising on Python to maximise their access to cutting-edge AI libraries, tools, and talent.</span></p><h3 class="c15" id="h.ofqgujays8vl"><span class="c9">OpenCL</span></h3><p class="c0"><span>We&#39;ve placed </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://www.khronos.org/opencl/&amp;sa=D&amp;source=editors&amp;ust=1752765587161440&amp;usg=AOvVaw0vDq2xn1Ab02exLej2PHHo">OpenCL</a></span><span class="c4">&nbsp;in the Hold ring of our Languages &amp; Frameworks quadrant. While OpenCL (Open Computing Language) was groundbreaking when introduced as a standard for parallel programming across different types of processors, we believe teams should look to alternatives for new projects.</span></p><p class="c0 c2"><span class="c4"></span></p><p class="c0"><span class="c4">Despite its promise of write-once-run-anywhere code for GPUs, CPUs, and other accelerators, OpenCL has seen declining industry support and faces significant challenges. Major hardware vendors have shifted their focus to more specialised frameworks like CUDA for NVIDIA hardware, while newer alternatives such as SYCL and modern GPU compute frameworks offer better developer experiences with similar cross-platform benefits. The complexity of the OpenCL programming model, combined with inconsistent tooling support and a fragmented ecosystem, makes it increasingly difficult to justify for new development compared to more actively maintained alternatives.</span></p><h1 class="c33" id="h.dmcxqwupfoqd"><span class="c41">Tools</span></h1><h2 class="c10" id="h.ugzs2xyc9ine"><span class="c29">Adopt</span></h2><h3 class="c15" id="h.7oma38kyuqdb"><span>Software Engineering Copilots</span></h3><p class="c11"><span>Software Engineering Copilots represent a new category of AI-powered development tools that act as intelligent coding assistants. These tools, including </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://www.cursor.com/&amp;sa=D&amp;source=editors&amp;ust=1752765587164888&amp;usg=AOvVaw2_9MVyu-jyDAKD9YxcEoBP">Cursor</a></span><span>, </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://github.com/features/copilot&amp;sa=D&amp;source=editors&amp;ust=1752765587165070&amp;usg=AOvVaw2RR_nMzsJmx_oRhSxdvWHh">Copilot</a></span><span>, </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://codeium.com/windsurf&amp;sa=D&amp;source=editors&amp;ust=1752765587165206&amp;usg=AOvVaw3CDm-EwAojTiTCt-l8iuV1">Windsurf</a></span><span>, </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://zed.dev/&amp;sa=D&amp;source=editors&amp;ust=1752765587165313&amp;usg=AOvVaw3eZBInB-J7o8PQjxgmjuVs">Zed</a></span><span>, </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://traycer.ai/&amp;sa=D&amp;source=editors&amp;ust=1752765587165474&amp;usg=AOvVaw148Zh5YnUJvsrScy_dHbjr">Traycer</a></span><span>, </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://sourcegraph.com/demo/cody&amp;sa=D&amp;source=editors&amp;ust=1752765587165605&amp;usg=AOvVaw1MnPFYLfUXRpmwNOSTceXL">Cody</a></span><span>, </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://github.com/cline/cline&amp;sa=D&amp;source=editors&amp;ust=1752765587165737&amp;usg=AOvVaw3XD6Gz-jIib9K0J3pQhpPH">Cline</a></span><span>, </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://www.tabnine.com/&amp;sa=D&amp;source=editors&amp;ust=1752765587165866&amp;usg=AOvVaw234bHVBrC4fL-UbpyjhSf6">Tabnine</a></span><span>&nbsp;and </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://github.com/Aider-AI/aider&amp;sa=D&amp;source=editors&amp;ust=1752765587165996&amp;usg=AOvVaw0eHLppKu9OkfZKUD7nH9Ld">Aider</a></span><span class="c4">&nbsp;either exist as standalone IDEs or integrate as plugins into existing IDEs, and offer code completion, refactoring suggestions, and automated implementation of routine tasks.</span></p><p class="c0"><span class="c4">We&#39;re seeing a clear pattern emerge in how these tools impact different experience levels. Counter-intuitively, senior engineers are deriving the most value by leveraging AI to accelerate well-understood tasks and automate routine code generation, whilst maintaining strict quality control over the output. Junior developers often struggle to effectively evaluate AI suggestions, sometimes accepting problematic implementations or failing to spot edge cases that weren&#39;t properly handled.</span></p><p class="c0 c2"><span class="c4"></span></p><p class="c0"><span class="c4">We&rsquo;ve placed Software Engineering Copilots firmly in the Adopt ring. The productivity gains (particularly for experienced developers who can effectively guide and evaluate AI suggestions) are substantial enough to justify this placement. Organisations report significant productivity improvements on routine coding tasks, with some teams achieving even more impressive results through careful integration into their workflows.</span></p><p class="c11"><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://www.cursor.com/&amp;sa=D&amp;source=editors&amp;ust=1752765587169076&amp;usg=AOvVaw2v8Zg-kFgg2KEF_zPFOFAO">Cursor</a></span><span>&nbsp;has emerged as a current frontrunner with its implementation of </span><span class="c30">.cursorrules</span><span class="c4">, which allows teams to share configuration settings and enforce consistent coding practices across projects. This feature enables organisations to codify their standards, architectural patterns, and security guidelines directly into the AI assistant, addressing many of our previous concerns about inconsistent output and quality control. While other tools will likely implement similar capabilities soon, Cursor&#39;s current implementation provides a robust framework for enterprise adoption.</span></p><p class="c11"><span class="c4">For teams adopting these tools, we still recommend a &quot;trust but verify&quot; approach: use AI assistance for initial implementation and routine tasks, but maintain rigorous code review and testing practices. Organisations should also consider providing structured training for junior developers on effectively collaborating with AI tools, focusing on developing the critical thinking skills needed to evaluate and refine AI-generated code.</span></p><p class="c11"><span class="c4">The rate of improvement in this space continues to be remarkable, with new capabilities being added regularly. Teams should remain flexible in their tool selection, as today&#39;s leader may be surpassed by innovations in competing products tomorrow. Regardless of the specific tool chosen, the fundamental shift towards AI-augmented development appears to be permanent, and organisations delaying adoption risk finding themselves at a competitive disadvantage.</span></p><h3 class="c15" id="h.l8ey8kh9ieei"><span class="c9">Provider-agnostic LLM facades</span></h3><p class="c11"><span>The LLM landscape evolves rapidly, making today&#39;s optimal choice potentially outdated within months. We recommend implementing a facade pattern between your application and LLM providers, rather than building directly against specific APIs. This approach reduces vendor lock-in and enables easier testing of alternative models as they emerge. When considering whether to write your own code, be sure to consider tools such as the lightweight </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://github.com/andrewyng/aisuite&amp;sa=D&amp;source=editors&amp;ust=1752765587174404&amp;usg=AOvVaw0N-SrDLUFyY221D-0HNi4R">AISuite</a></span><span>, Simon Willison&rsquo;s </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://github.com/simonw/llm&amp;sa=D&amp;source=editors&amp;ust=1752765587174654&amp;usg=AOvVaw0p3lH-yOrsJV0eSTII9e9b">LLM</a></span><span>&nbsp;library and CLI tool, or heavyweight alternatives such as </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://www.langchain.com/&amp;sa=D&amp;source=editors&amp;ust=1752765587174936&amp;usg=AOvVaw2f86DpRfjJ0AGFfkpmtDLF">LangChain</a></span><span>&nbsp;and </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://www.llamaindex.ai/&amp;sa=D&amp;source=editors&amp;ust=1752765587175093&amp;usg=AOvVaw1ClWQIGqUMUOx49yUifyJS">LlamaIndex</a></span><span>.</span></p><p class="c11"><span class="c4">This recommendation reflects our team&#39;s experience seeing projects hampered by tight coupling to specific LLM providers, and the subsequent maintenance burden when transitioning to newer, more capable models.</span></p><h3 class="c15" id="h.6zjxm26cqll4"><span class="c9">Notebooks</span></h3><p class="c11"><span class="c4">We&#39;ve placed Notebooks in the Adopt ring because they have become the de facto standard for data science and machine learning experimentation, prototyping, and documentation. The interactive nature of notebooks, combining code execution with rich text explanations and visualisations, makes them particularly valuable for AI/ML workflows where iterative exploration and clear documentation of model development are essential.</span></p><p class="c11"><span class="c4">Widespread adoption across both industry and academia, plus an extensive plugin ecosystem and integration with popular AI frameworks, demonstrates their maturity as a method of interacting with code. We especially value how notebooks facilitate collaboration between technical and non-technical team members, as they can serve as living documents that combine business requirements, technical implementation, and results in a single, shareable format.</span></p><p class="c0"><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://jupyter.org/&amp;sa=D&amp;source=editors&amp;ust=1752765587178262&amp;usg=AOvVaw3Fbr_o8EebCbcvEcF2c30g">Jupyter</a></span><span>&nbsp;notebooks are the most widely used, supporting multiple languages including Python, R and Julia. The cloud platforms provide their own implementations: </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://colab.research.google.com/&amp;sa=D&amp;source=editors&amp;ust=1752765587178834&amp;usg=AOvVaw3gR9povBMj-hecnrb3vg3u">Google Colab</a></span><span>, AWS </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://aws.amazon.com/sagemaker-ai/notebooks/&amp;sa=D&amp;source=editors&amp;ust=1752765587179035&amp;usg=AOvVaw2R8XPNPlQiWZ7vYtrMsw19">Sagemaker Notebooks</a></span><span>, </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://learn.microsoft.com/en-gb/azure/machine-learning/how-to-run-jupyter-notebooks&amp;sa=D&amp;source=editors&amp;ust=1752765587179260&amp;usg=AOvVaw3SvEtPHTuzT5eQZKBhvjUs">Azure Notebooks</a></span><span>, </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://docs.databricks.com/en/notebooks/index.html&amp;sa=D&amp;source=editors&amp;ust=1752765587179471&amp;usg=AOvVaw30l57FM6_QCQXQeWkVR2zZ">Databricks Notebooks</a></span><span>. And there are language specific notebooks, such as </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=http://pluto.jl&amp;sa=D&amp;source=editors&amp;ust=1752765587179739&amp;usg=AOvVaw3pRThHWC8tQvVPoRby4EZy">Pluto.jl</a></span><span>&nbsp;for Julia, </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://github.com/nextjournal/clerk&amp;sa=D&amp;source=editors&amp;ust=1752765587179905&amp;usg=AOvVaw0GrX0GjQIOuBHyDj_K9K_d">Clerk</a></span><span>&nbsp;for Clojure, </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://github.com/polynote/polynote&amp;sa=D&amp;source=editors&amp;ust=1752765587180080&amp;usg=AOvVaw3yELXD35WeHl5oRbQv2W27">Polynote</a></span><span class="c4">&nbsp;for Scala.</span></p><p class="c0 c2"><span class="c4"></span></p><h2 class="c10" id="h.invvqeqjvrp1"><span class="c29">Trial</span></h2><h3 class="c15" id="h.z0xxp0axjpep"><span>Agentic </span><span>Computer Use</span></h3><p class="c11"><span>We&#39;re seeing AI agents being developed for increasingly complex and varied environments, such as web agents that can browse and interact with online content like </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://operator.chatgpt.com/&amp;sa=D&amp;source=editors&amp;ust=1752765587184004&amp;usg=AOvVaw0IeF68-w7-3llD1YgNGKbA">OpenAI&rsquo;s Operator</a></span><span>&nbsp;to systems that interact with a user&rsquo;s whole operating system such as </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://docs.anthropic.com/en/docs/build-with-claude/computer-use&amp;sa=D&amp;source=editors&amp;ust=1752765587184554&amp;usg=AOvVaw1j-EEZvw-fvSC-Xar0YdTW">Claude Computer Use</a></span><span class="c4">. Each agent is designed with specific capabilities and constraints suited to its operational environment: a RAG (Retrieval Augmented Generation) agent might be optimised for searching and synthesising internal documentation, while a development environment agent needs deep understanding of codebases and development workflows.</span></p><p class="c11"><span>A particularly interesting trend is the emergence of agents that operate across entire computing environments such as </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://devin.ai/&amp;sa=D&amp;source=editors&amp;ust=1752765587185976&amp;usg=AOvVaw1iugKc2rVPpxWaHv8TbK4F">Devin</a></span><span class="c4">, with the ability to execute terminal commands, manipulate files, and interact with development tools. This expanded scope brings both opportunities and challenges: while these agents can potentially automate complex workflows, they require careful consideration around access controls, security boundaries, and failure recovery mechanisms.</span></p><p class="c11"><span class="c4">We&#39;ve placed AI Agents in the Trial ring. While we&#39;re seeing promising early applications, particularly in constrained environments like RAG systems, the technology is still evolving rapidly and best practices around security, reliability, and control mechanisms are still emerging. The potential for both value and risk is significant, warranting close attention but careful consideration before widespread adoption.</span></p><p class="c11"><span class="c4">We&#39;re seeing a clear pattern of successful implementations when agents are given well-defined boundaries and carefully curated tool access. Organisations are reporting meaningful productivity gains, particularly in areas like document processing, customer service, and development workflows. The ability to chain together multiple operations while maintaining context is proving valuable in many scenarios.</span></p><p class="c11"><span class="c4">However, we&#39;re also observing significant challenges around reliability, security, and control. Agents can sometimes get stuck in loops, make incorrect tool selections, or fail to properly handle edge cases. There are also important questions around audit trails, permissions management, and failure recovery that need to be addressed. Until these concerns are more fully resolved, we recommend careful assessment and controlled trials rather than widespread adoption.</span></p><h2 class="c10" id="h.ocybg9tcut5j"><span class="c29">Assess</span></h2><h3 class="c15" id="h.r8jmeqirbult"><span>AI Application Bootstrappers</span></h3><p class="c0"><span>We have placed AI Application Bootstrappers like </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://v0.dev/&amp;sa=D&amp;source=editors&amp;ust=1752765587190997&amp;usg=AOvVaw08czMls_jCMrTO6yxvazh6">V0</a></span><span>, </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://bolt.new/&amp;sa=D&amp;source=editors&amp;ust=1752765587191162&amp;usg=AOvVaw0bruBBxCHdEabbtoUXkQHE">Bolt.new</a></span><span>&nbsp;and </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://replit.com/ai&amp;sa=D&amp;source=editors&amp;ust=1752765587191303&amp;usg=AOvVaw19_C6W4AXZujSBKucZ2Xkb">Replit Agent</a></span><span class="c4">&nbsp;in the Assess ring of our Tools quadrant. These tools represent an intriguing new approach to rapidly generating complete applications from prompts or designs. While they can dramatically accelerate the creation of demos and proofs of concept, their current limitations lead us to recommend careful assessment before adoption.</span></p><p class="c0 c2"><span class="c4"></span></p><p class="c0"><span class="c4">The primary value proposition is clear: the ability to go from concept to working prototype in hours instead of days or weeks. However, our experience shows that success with these tools correlates strongly with existing software engineering expertise. Senior developers can effectively use them as accelerators, understanding how to refactor the generated code, identify potential issues, and establish proper architectural boundaries. In contrast, junior developers or non-technical users often struggle with maintaining and evolving the generated codebase, finding themselves unable to effectively debug issues or make substantial modifications without creating cascading problems.</span></p><p class="c0 c2"><span class="c4"></span></p><p class="c0"><span class="c4">While these tools excel at creating initial implementations, the significant effort required to make applications production-ready still requires substantial engineering knowledge. We&#39;re particularly concerned about teams using bootstrapped code as a foundation for production systems without the expertise to properly evaluate and refactor the generated codebase. The tools are promising but should be approached with clear understanding of their current limitations and best used by teams with strong software engineering fundamentals.</span></p><p class="c0 c2"><span class="c4"></span></p><p class="c0"><span class="c4">Looking ahead, we expect these tools to mature and potentially move into the Trial ring as they develop better guardrails and more maintainable output. For now, we recommend assessing them primarily for simple prototyping and proof-of-concept work, while maintaining careful separation between bootstrapped demos and production codebases.</span></p><h2 class="c10" id="h.qlkaf37fyy4h"><span class="c29">Hold</span></h2><h3 class="c15" id="h.x1b3f1aayjhj"><span class="c9">Conversational data analysis</span></h3><p class="c11"><span class="c4">We&#39;ve placed conversational data analysis tools in the Hold ring as this emerging category shows promise but requires careful evaluation at present. These tools aim to enable data analysis through natural language interactions rather than writing code directly.</span></p><p class="c11"><span>Tools such as </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://github.com/sinaptik-ai/pandas-ai&amp;sa=D&amp;source=editors&amp;ust=1752765587197812&amp;usg=AOvVaw3rMWYovpWdTmp4GbQSHyxL">pandas-ai</a></span><span>, </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://github.com/tablegpt/tablegpt-agent&amp;sa=D&amp;source=editors&amp;ust=1752765587198031&amp;usg=AOvVaw3qprQDS7yVa7lfMqg9ZBFP">tablegpt</a></span><span>, </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://promptql.hasura.io/&amp;sa=D&amp;source=editors&amp;ust=1752765587198173&amp;usg=AOvVaw0taBEY8CQ66eRIn0iduxrF">promptql</a></span><span>, </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://julius.ai/&amp;sa=D&amp;source=editors&amp;ust=1752765587198295&amp;usg=AOvVaw0v6aw3xaeHtetFi2u-OZ13">Julius</a></span><span>, and </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://www.gathr.ai/data-engineering/&amp;sa=D&amp;source=editors&amp;ust=1752765587198505&amp;usg=AOvVaw0EhKql4iOMFMZQzWTX-QS-">Gathr&rsquo;s data engineering</a></span><span>&nbsp;allow analysts to query datasets using plain English, generating code behind the scenes. While this could potentially democratise data analysis and data engineering, we have concerns about the reliability and predictability of the generated code. The AI can sometimes misinterpret requests or misrepresent results. For instance, Uber has documented its internal </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://www.uber.com/en-GB/blog/query-gpt/&amp;sa=D&amp;source=editors&amp;ust=1752765587199600&amp;usg=AOvVaw05D4_PQq-7E1u7DYUuX3HP">QueryGPT</a></span><span class="c4">&nbsp;tool, highlighting the significant number of example queries and guardrails required to achieve acceptable reliability.</span></p><p class="c11"><span class="c4">We&#39;re also monitoring how these tools handle data privacy, given they often require sending queries to external AI services. For teams considering these tools, we recommend starting with non-sensitive datasets and asking analytics questions to which you already know the answers.</span></p><h1 class="c33" id="h.h8nbngj6u8tr"><span class="c41">Platforms</span></h1><h2 class="c10" id="h.5ht4er9vpmx4"><span class="c29">Adopt</span></h2><h3 class="c15" id="h.vrsao09mz4xy"><span class="c9">Weights &amp; Biases</span></h3><p class="c11"><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://wandb.ai/site/&amp;sa=D&amp;source=editors&amp;ust=1752765587201430&amp;usg=AOvVaw3si-W0Dk5P2m2EC0mT7qYF">Weights &amp; Biases</a></span><span class="c4">&nbsp; is a platform designed for tracking and visualising machine learning experiments. In recent projects, we&#39;ve observed that it provides a robust solution for managing machine learning workflows, particularly when dealing with complex models and large datasets. Its user-friendly interface and integration capabilities with popular machine learning libraries make it accessible for teams looking to improve their model development processes.</span></p><p class="c11"><span class="c4">We&#39;ve seen how systems such as Weights &amp; Biases can catalyse positive cultural changes in ML teams. By making experiment tracking very light touch, requiring just a few lines of code, they remove the friction that sometimes prevents teams from maintaining good measurement practices. When tracking experiments becomes a natural part of the workflow rather than an extra burden, teams tend to measure more, compare results more frequently, and generally make more data-driven decisions.</span></p><p class="c11"><span class="c4">Collaboration features such as shared dashboards and reports amplify these benefits by making results and insights visible to the whole team. Rather than knowledge being siloed in individual notebooks or spreadsheets, experiments become shared assets that everyone can learn from. This visibility often leads to more discussion about results, faster knowledge sharing, and ultimately quicker iteration cycles as teams build upon each other&#39;s work rather than inadvertently duplicating efforts. However, it&#39;s important to note that tool adoption alone isn&#39;t enough&mdash;teams need to actively foster a culture that values measurement and experimentation for these benefits to fully materialise.</span></p><h3 class="c15" id="h.xplxquryxfxx"><span class="c9">Foundation models</span></h3><p class="c11"><span class="c4">Foundation model providers continue to evolve at a rapid pace. Major players like OpenAI, Anthropic, Google, and Meta compete alongside emerging organisations such as DeepSeek, Alibaba, IBM and others. While industry benchmarks help compare these models, they tell only part of the story: different models excel in different areas, and benchmark results should be viewed as indicative rather than definitive.</span></p><p class="c11"><span class="c4">A clear trend has emerged in how providers differentiate their offerings across three distinct tiers: smaller, faster models (e.g., Claude Haiku, DeepSeek Coder, Qwen Turbo) optimised for speed and cost; larger, more capable models (e.g., Claude Sonnet, DeepSeek V3, Qwen Max) balancing capabilities with reasonable response times; and specialised reasoning models (e.g., Claude Sonnet Extended, OpenAI o1, DeepSeek R1) designed for complex problem-solving. These reasoning models consume significantly more tokens and command higher per-token costs, but demonstrate remarkable capabilities in solving challenging logical puzzles, mathematics problems, and coding tasks.</span></p><p class="c11"><span class="c4">We believe foundation models have matured enough to warrant adoption for many business applications. When paired with appropriate infrastructure (few-shot prompting, guardrails, retrieval-augmented generation, and evaluation frameworks), they offer compelling solutions to a wide range of problems. Our experience suggests there&#39;s no universal &quot;best model&quot;. We recommend implementing your own benchmarking process focused on your specific use cases. When selecting a model, consider factors beyond raw performance, such as pricing, reliability, data privacy requirements, and whether on-premise deployment is needed. The recent emergence of high-quality open-source models with permissive licensing (such as DeepSeek&#39;s offerings) provides additional options for organisations with specific security or deployment requirements.</span></p><p class="c11"><span class="c4">Key Considerations:</span></p><ul class="c42 lst-kix_v8gw1emso66v-0 start"><li class="c11 c18 li-bullet-0"><span class="c4">Performance &amp; capabilities (accuracy, speed, and domain-specific strengths)</span></li><li class="c11 c18 li-bullet-0"><span class="c4">Total cost of ownership (API costs, compute resources, and integration)</span></li><li class="c11 c18 li-bullet-0"><span class="c4">Deployment options &amp; technical requirements (cloud, self-hosted, edge)</span></li><li class="c11 c18 li-bullet-0"><span class="c4">Data privacy &amp; compliance (regulatory, legal, and security implications)</span></li><li class="c11 c18 li-bullet-0"><span class="c4">Integration &amp; lifecycle management (context limitations, version control, updates)</span></li><li class="c11 c18 li-bullet-0"><span class="c4">Vendor stability &amp; support (roadmap alignment, documentation, community)</span></li></ul><p class="c1 c0"><span class="c39">Foundation Model Providers Feature Comparison (April 2025)</span></p><table class="c38"><thead><tr class="c23"><td class="c7" colspan="1" rowspan="1"><p class="c16"><span class="c4">Provider</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c16"><span class="c4">Reasoning Models</span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c16"><span class="c4">Multimodal</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c16"><span class="c4">Self-hosting</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c16"><span class="c4">Open Weights</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c16"><span class="c4">Long Context</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c16"><span class="c4">RAG Optimised</span></p></td><td class="c27" colspan="1" rowspan="1"><p class="c16"><span class="c4">Multilingual</span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c16"><span class="c4">Edge Deployment</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c16"><span class="c4">Real-time Data</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c16"><span class="c4">Enterprise Focus</span></p></td><td class="c36" colspan="1" rowspan="1"><p class="c16"><span class="c4">Model Selection Link</span></p></td><tbody></tbody></tr><tr class="c21"><td class="c7" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">OpenAI</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c27" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c36" colspan="1" rowspan="1"><p class="c1 c0"><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://platform.openai.com/docs/models&amp;sa=D&amp;source=editors&amp;ust=1752765587221941&amp;usg=AOvVaw2zqAUoLWNHiSk_w4ZwojaU">Models</a></span></p></td></tr><tr class="c21"><td class="c7" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">Anthropic</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c27" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c36" colspan="1" rowspan="1"><p class="c1 c0"><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://docs.anthropic.com/en/docs/about-claude/models/all-models&amp;sa=D&amp;source=editors&amp;ust=1752765587225846&amp;usg=AOvVaw2shYwfIoHReCW6V53WuFMC">Models</a></span></p></td></tr><tr class="c21"><td class="c7" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">DeepSeek</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c27" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c36" colspan="1" rowspan="1"><p class="c1 c0"><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://api-docs.deepseek.com/quick_start/pricing&amp;sa=D&amp;source=editors&amp;ust=1752765587229626&amp;usg=AOvVaw32KkSzh4H3Puh4t6VqSsrD">Models</a></span></p></td></tr><tr class="c21"><td class="c7" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">Meta</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c27" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c36" colspan="1" rowspan="1"><p class="c1 c0"><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://www.llama.com/docs/model-cards-and-prompt-formats/&amp;sa=D&amp;source=editors&amp;ust=1752765587233878&amp;usg=AOvVaw3yZfSJ_z4tWuu8Zi-t60J9">Models</a></span></p></td></tr><tr class="c28"><td class="c7" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">Alibaba (Qwen)</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c27" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c36" colspan="1" rowspan="1"><p class="c1 c0"><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://www.alibabacloud.com/help/en/model-studio/model-user-guide/&amp;sa=D&amp;source=editors&amp;ust=1752765587237892&amp;usg=AOvVaw1MN1f_Tcn5JoKPZtebJ7Vl">Models</a></span></p></td></tr><tr class="c21"><td class="c7" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">AWS</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c27" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c36" colspan="1" rowspan="1"><p class="c1 c0"><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://aws.amazon.com/ai/generative-ai/nova/&amp;sa=D&amp;source=editors&amp;ust=1752765587241962&amp;usg=AOvVaw1yrAMBMO_Z-hggbdBnt9Xe">Models</a></span></p></td></tr><tr class="c23"><td class="c7" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">X (formerly Twitter)</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c27" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c36" colspan="1" rowspan="1"><p class="c1 c0"><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://docs.x.ai/docs/models&amp;sa=D&amp;source=editors&amp;ust=1752765587245868&amp;usg=AOvVaw1QiOFa_uP-fQur4O8obWZA">Models</a></span></p></td></tr><tr class="c28"><td class="c7" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">Mistral AI</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c0 c1"><span class="c4">&#10003;</span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c27" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c36" colspan="1" rowspan="1"><p class="c1 c0"><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://docs.mistral.ai/getting-started/models/models_overview/&amp;sa=D&amp;source=editors&amp;ust=1752765587249764&amp;usg=AOvVaw0sAiyyW690hJGuKJX0ONKV">Models</a></span></p></td></tr><tr class="c21"><td class="c7" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">Google</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c27" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c1 c0 c2"><span class="c4"></span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c1 c0"><span class="c4">&#10003;</span></p></td><td class="c36" colspan="1" rowspan="1"><p class="c1 c0"><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://ai.google.dev/gemini-api/docs/models&amp;sa=D&amp;source=editors&amp;ust=1752765587253878&amp;usg=AOvVaw25_6ecA0viZ-fPx0Xoywos">Models</a></span></p></td></tr></thead></table><p class="c1 c0"><span class="c39">Feature Definitions</span></p><ul class="c42 lst-kix_5ee0xgh37a3n-0 start"><li class="c0 c18 li-bullet-0"><span class="c4">Open Weights: Models whose weights are publicly available for download and customisation</span></li><li class="c0 c18 li-bullet-0"><span class="c4">Self-hosting: Ability to run models on your own infrastructure</span></li><li class="c0 c18 li-bullet-0"><span class="c4">Reasoning Models: Specialised models for complex reasoning tasks like mathematics or step-by-step problem solving</span></li><li class="c0 c18 li-bullet-0"><span class="c4">Multimodal: Support for multiple input/output modalities (text, images, audio, etc.)</span></li><li class="c0 c18 li-bullet-0"><span class="c4">Long Context: Support for context windows of 100K tokens or more</span></li><li class="c0 c18 li-bullet-0"><span class="c4">RAG Optimised: Perform well at generating coherent responses that integrate external knowledge from retrieved documents with model knowledge, might include specific capabilities like improved citation of sources</span></li><li class="c0 c18 li-bullet-0"><span class="c4">Enterprise Focus: Strong emphasis on governance, security, and enterprise integration</span></li><li class="c0 c18 li-bullet-0"><span class="c4">Multilingual: Strong support for multiple languages beyond English</span></li><li class="c0 c18 li-bullet-0"><span class="c4">Edge Deployment: Optimised for deployment on edge devices or resource-constrained environments</span></li><li class="c0 c18 li-bullet-0"><span class="c4">Real-time Data: Access to real-time (or very recent) information</span></li></ul><h2 class="c10" id="h.uty2xcx8qxs3"><span class="c29">Trial</span></h2><h3 class="c15" id="h.ktj5pluzzcbq"><span class="c9">MLflow</span></h3><p class="c11"><span>We have placed </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://mlflow.org/&amp;sa=D&amp;source=editors&amp;ust=1752765587258781&amp;usg=AOvVaw1yo9jSM40OHouy14ABNtKq">MLFlow</a></span><span class="c4">&nbsp; in the Trial ring of the Platforms quadrant due to its potential as a lightweight and modular option for teams seeking to manage the machine learning lifecycle. Its open-source nature makes it an attractive alternative to the more monolithic cloud-based MLOps platforms provided by vendors like AWS, Microsoft and Google. A key advantage of MLFlow is its ability to avoid vendor lock-in, offering teams the flexibility to maintain control of their infrastructure and adapt workflows as their needs evolve.</span></p><p class="c11"><span>That said, realising the benefits of MLFlow requires teams to have a certain level of technical expertise to configure and integrate it into their existing systems effectively. Unlike cloud-native behemoths such as </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://aws.amazon.com/sagemaker/&amp;sa=D&amp;source=editors&amp;ust=1752765587261041&amp;usg=AOvVaw0py8xAnIy5Qevii7uZzLdn">SageMaker</a></span><span>&nbsp;or </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://cloud.google.com/vertex-ai&amp;sa=D&amp;source=editors&amp;ust=1752765587261244&amp;usg=AOvVaw3s4JTbPBskPOGQkl9U6XBA">Vertex AI</a></span><span class="c4">, MLFlow does not provide an all-in-one, plug-and-play experience. Instead, it offers modular components that must be tailored to specific use cases. We recommend assessing MLFlow if your organisation values flexibility, has the technical proficiency to manage integrations, and prefers avoiding dependency on proprietary platforms early in your MLOps journey.</span></p><h3 class="c15" id="h.7ywpvb7qyv32"><span class="c9">Open weight LLMs</span></h3><p class="c11"><span>2024 was the year when open weight LLMs (which are sometimes incorrectly referred to as &lsquo;open source&rsquo;) from companies such as </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://www.llama.com/&amp;sa=D&amp;source=editors&amp;ust=1752765587262719&amp;usg=AOvVaw2zLb7xIkk7HDhQP7sLidlx">Meta</a></span><span>&nbsp;and </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://www.deepseek.com/&amp;sa=D&amp;source=editors&amp;ust=1752765587262873&amp;usg=AOvVaw1j7OMd8qqbARU2XpG0pcux">Deepseek</a></span><span class="c4">&nbsp;reached maturity, with some even surpassing flagship frontier models on certain tasks. We&#39;ve placed open weight LLMs in the Trial ring because they allow organisations to benefit from AI capabilities while maintaining control over their data and deployment. These models have demonstrated impressive performance, particularly in specialised domains when fine-tuned on specific tasks.</span></p><p class="c11"><span class="c4">The key benefits include reduced operational costs compared to API-based services, full control over model deployment and customisation, and the ability to run models in air-gapped environments where data privacy is paramount. However, we&#39;ve kept them in Trial because organisations need considerable ML engineering expertise to deploy and maintain these models effectively, and the total cost of ownership isn&#39;t always lower than API-based alternatives when accounting for computational resources and engineering time.</span></p><p class="c11"><span class="c4">For certain use cases, the simplicity of a pay-per-use API integration outweighs the benefits and greater control of hosting an open source LLM. Additionally, implementing appropriate security controls, prompt injection protection, and data governance poses significant challenges.</span></p><h3 class="c15" id="h.nqam9a9mf12r"><span class="c9">Lakera</span></h3><p class="c11"><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://www.lakera.ai/&amp;sa=D&amp;source=editors&amp;ust=1752765587266732&amp;usg=AOvVaw1eUbHmgH_TrBbR7_4hfDcs">Lakera</a></span><span class="c4">&nbsp;is an AI safety and robustness platform designed to detect and mitigate risks in machine learning systems. It provides mechanisms for testing, analysis, and quality assurance to help developers identify weaknesses or vulnerabilities in AI/ML models prior to deployment. This makes it particularly appealing in contexts where reliability and safety are paramount, such as finance, healthcare, or any domain subject to compliance constraints.</span></p><p class="c11"><span>We have placed Lakera in the </span><span>Trial</span><span class="c4">&nbsp;ring because its value is clearly demonstrated in scenarios where established safety practices need to be extended to AI systems. Its ability to identify edge cases and potential failures adds a layer of assurance that is crucial for risk-sensitive operations. However, as a newer and relatively niche platform, it is still finding its place within mainstream AI development processes. Teams looking to leverage its capabilities will need to have a certain level of maturity in their own testing and validation workflows.</span></p><p class="c11"><span class="c4">For now, Lakera stands out as a focused and valuable tool for teams committed to addressing AI risk in the most responsible way. We recommend conducting small-scale experiments to assess its suitability for your specific requirements before considering broader adoption.</span></p><h3 class="c15" id="h.w2wfuo9xb4eq"><span class="c9">Vector Databases</span></h3><p class="c11"><span>Vector databases have emerged as specialised tools for managing the high-dimensional data representations (embeddings) required by AI models. They enable efficient similarity search across text, images, and other content types. Prominent solutions include </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://www.pinecone.io/&amp;sa=D&amp;source=editors&amp;ust=1752765587271370&amp;usg=AOvVaw18f_x1SPgSu3HY_bgj8Vu3">Pinecone</a></span><span>, </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://qdrant.tech/&amp;sa=D&amp;source=editors&amp;ust=1752765587271621&amp;usg=AOvVaw1Wuv2aPrIq5AWRkGOg7yfz">Qdrant</a></span><span>, </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://milvus.io/&amp;sa=D&amp;source=editors&amp;ust=1752765587271815&amp;usg=AOvVaw07Kv7wfAaQvzg2DA6UuE0j">Milvus 2.0</a></span><span>&nbsp;and </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://weaviate.io/&amp;sa=D&amp;source=editors&amp;ust=1752765587271973&amp;usg=AOvVaw3hjRM-f_vXNhjG1BL9C9yh">Weaviate</a></span><span class="c4">.</span></p><p class="c11"><span>We&rsquo;ve generally placed vector databases in the </span><span>Trial</span><span>&nbsp;ring, as they have proven valuable for specific use cases such as semantic search and recommendation systems. However, their adoption should be carefully evaluated based on individual requirements. Traditional databases may be sufficient for simpler operations and typically require less coordination. Alternative approaches, such as Timescale&rsquo;s &nbsp;</span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://github.com/timescale/pgai&amp;sa=D&amp;source=editors&amp;ust=1752765587273241&amp;usg=AOvVaw0bAs4-0mMV7o9qmQQorBfP">PGAI vectorizer</a></span><span class="c4">, bring vector embedding search directly into the Postgres database, ensuring embeddings remain synchronised with underlying content changes.</span></p><p class="c11"><span class="c4">If a vector database is required for your use case, the choice of provider often depends on factors such as scale requirements, the need for real-time updates, and whether a managed or self-hosted solution is preferred. Pinecone leads in production readiness but comes with the costs of a managed service, while open-source alternatives like Qdrant and Milvus offer greater control but demand more operational expertise.</span></p><h2 class="c10" id="h.lo4rf6hbcuhh"><span class="c29">Assess</span></h2><h3 class="c15" id="h.6m9t6zif3fot"><span class="c9">Crew.ai</span></h3><p class="c11"><span>We&#39;ve placed </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=http://crew.ai&amp;sa=D&amp;source=editors&amp;ust=1752765587275269&amp;usg=AOvVaw1BsCJ2kInceDwp34wL79B2">Crew.ai</a></span><span class="c4">&nbsp;in the Assess ring of the Platforms radiant because it represents a promising approach to multi-agent orchestration that&#39;s gaining traction among developers building complex AI systems.</span></p><p class="c11"><span class="c4">Crew.ai provides a framework for creating teams of specialised AI agents that work together to accomplish tasks through coordinated effort. Our team members report that it offers a well-structured approach to defining agent roles, communication patterns, and task delegation: addressing many of the challenges involved in building effective agentic systems. The framework&#39;s emphasis on human-in-the-loop integration, along with the ability to combine specialised agents with different capabilities, makes it particularly valuable for complex workflows where single-agent solutions fall short.</span></p><p class="c11"><span class="c4">While Crew.ai shows significant promise and has already been used successfully in production environments, we&#39;ve placed it in Assess rather than Trial because the multi-agent paradigm itself is still evolving. Organisations need to carefully evaluate whether the added complexity of managing multiple agents offers sufficient benefits over simpler approaches for their specific use cases. Teams should also be aware that best practices for agent collaboration are still emerging, and implementations may require considerable tuning and oversight to achieve reliable results.</span></p><p class="c11 c2"><span class="c4"></span></p><h3 class="c11 c35" id="h.hgcbai9fd103"><span class="c9">Galileo.ai</span></h3><p class="c11"><span>We&#39;ve placed </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=http://galileo.ai&amp;sa=D&amp;source=editors&amp;ust=1752765587279396&amp;usg=AOvVaw3nXHURfEhb-xILh4GKc0w0">Galileo.ai</a></span><span class="c4">&nbsp;in the Assess ring of the Platforms radiant because it represents an interesting approach to evaluating and improving AI model performance. It deserves attention but requires careful consideration before being adopted more broadly.</span></p><p class="c11"><span class="c4">Galileo.ai offers tools for understanding, measuring, and refining the performance of large language models, providing visibility into model behaviour through data exploration and analytics. Our committee has noted that teams using the platform report better insights into how their AI systems perform across different scenarios and edge cases. It shows promise in helping developers identify and fix issues such as hallucinations, biases, and performance degradation, which are often difficult to detect through traditional testing methods.</span></p><p class="c11"><span>We recommend assessing this platform, particularly if your organisation is developing custom models or fine-tuning existing ones, as the insights it provides could significantly improve model quality. However, we&#39;ve stopped short of recommending it for trial by all teams, as its value varies depending on your level of AI maturity and your specific use cases. Organisations with simpler AI implementations, or those primarily using out-of-the-box models, may find less immediate benefit. The platform is likely to offer the most value to organisations that are actively developing or fine-tuning models, or deploying AI in high-stakes environments where consistent performance is critical. Teams should also consider whether they have the technical resources required to act effectively on the insights the platform provides.</span></p><h3 class="c11 c35" id="h.ixmfl6t1ahoy"><span class="c9">Kubeflow</span></h3><p class="c11"><span>We&#39;ve placed </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://www.kubeflow.org/&amp;sa=D&amp;source=editors&amp;ust=1752765587283908&amp;usg=AOvVaw2T8Pmemn3VHU6DYw_syHeF">Kubeflow</a></span><span class="c4">&nbsp;in the Assess ring of our Platforms quadrant. This open-source machine learning platform, built on Kubernetes, offers a comprehensive solution for managing ML workflows, but it requires careful evaluation before widespread adoption.</span></p><p class="c11"><span class="c4">Kubeflow is gaining traction among data science and MLOps teams looking to standardise their machine learning workflows. Its strength lies in combining Kubernetes&#39; orchestration capabilities with ML-specific tools: Pipelines for workflow automation, Katib for hyperparameter tuning, and KFServing for model deployment. This integrated approach helps bridge the gap between data scientists and operations teams, addressing one of the core challenges in operationalising ML models.</span></p><p class="c11"><span class="c4">However, several factors keep Kubeflow in our Assess ring. First, implementing Kubeflow demands significant expertise in both Kubernetes and ML engineering&mdash;a specialised skill set that remains relatively uncommon. Second, while the platform is maturing, we&#39;ve observed that many organisations struggle with its complexity during initial setup and ongoing maintenance. Teams often report a steep learning curve before realising tangible benefits.</span></p><p class="c11"><span>Organisations with established ML practices and existing Kubernetes expertise should consider assessing Kubeflow, particularly if they&#39;re facing challenges with ML model deployment, experiment reproducibility or resource utilisation. The platform is especially suited to enterprises managing multiple ML models in production that require systematic oversight across their lifecycle. Smaller teams, or those earlier in their ML journey, may want to explore simpler alternatives first or consider managed options like </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://cloud.google.com/vertex-ai/docs/pipelines/introduction&amp;sa=D&amp;source=editors&amp;ust=1752765587288616&amp;usg=AOvVaw1cRKntanEAhFIazBdIrP6x">Vertex AI Pipelines</a></span><span class="c4">, which abstract away some of the infrastructure complexity.</span></p><h2 class="c10" id="h.8ffosw25fu45"><span class="c29">Hold</span></h2><h3 class="c15" id="h.1trqf7o161xt"><span class="c9">Building against vendor-specific APIs</span></h3><p class="c11"><span class="c4">We&#39;ve placed &quot;Building against vendor-specific APIs&quot; in the Hold ring of the Platforms quadrant because tightly coupling your applications to vendor-specific LLM APIs poses significant business risks in this rapidly evolving landscape.</span></p><p class="c11"><span>The </span><span>foundation model ecosystem is changing at breakneck speed</span><sup><a href="#cmnt4" id="cmnt_ref4">[d]</a></sup><span class="c4">, with model capabilities, pricing and even entire companies shifting dramatically from month to month. Organisations that build directly against OpenAI, Anthropic or other proprietary APIs often find themselves locked in, facing painful migrations when a better or more cost-effective model emerges. We&#39;ve seen teams invest substantial engineering effort into rewriting API integrations after discovering their chosen vendor has been outperformed or has significantly increased its pricing.</span></p><p class="c11"><span>Instead, we recommend using abstraction libraries that provide a common interface to multiple LLM providers. Libraries such as </span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://github.com/andrewyng/aisuite&amp;sa=D&amp;source=editors&amp;ust=1752765587292065&amp;usg=AOvVaw0VNLr0ZzTiNnkw35aygnb6">AISuite</a></span><span>&nbsp;or Simon Willison&#39;s</span><span class="c8"><a class="c6" href="https://www.google.com/url?q=https://github.com/simonw/llm&amp;sa=D&amp;source=editors&amp;ust=1752765587292394&amp;usg=AOvVaw2Y6ydKqAYygX4BMAUjhty9">&nbsp;LLM CLI</a></span><span class="c4">&nbsp;let you switch between different models with minimal code changes, sometimes just a configuration update. These libraries handle the nuances of different vendor APIs, managing context windows, token limitations and provider-specific parameters behind a consistent interface. This approach preserves your flexibility to take advantage of new capabilities or improved pricing as the market evolves, while significantly reducing the engineering effort required to switch between models.</span></p><p class="c11"><span>These abstractions do add some complexity and may occasionally limit access to vendor-specific features, but in our view, the protection against vendor lock-in far outweighs these drawbacks in most cases. As the foundation model market continues to consolidate, maintaining the flexibility to adapt quickly will be crucial for both cost management and staying competitive.</span></p><div class="c13"><p class="c32"><a href="#cmnt_ref1" id="cmnt1">[a]</a><span class="c4">How do we feel about GraphRAG - seems related?&nbsp;</span></p><p class="c32"><span class="c4">https://arxiv.org/pdf/2404.16130</span></p></div><div class="c13"><p class="c32"><a href="#cmnt_ref2" id="cmnt2">[b]</a><span class="c4">good point - seems things are still too volatile to standardise on a particular model</span></p></div><div class="c13"><p class="c32"><a href="#cmnt_ref3" id="cmnt3">[c]</a><span class="c4">This was written with production implementation in mind. I think it needs a redraft to incorporate the emerging pattern of development-time tool use as part of the software engineering workflow</span></p></div><div class="c13"><p class="c32"><a href="#cmnt_ref4" id="cmnt4">[d]</a><span class="c4">great phrasing</span></p></div></body></html>